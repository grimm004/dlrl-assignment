{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "## Learning to play Gravitar with a Deep-Q Network\n",
    "\n",
    "### Abstract\n",
    "This is a Deep Convolutional Dueling Q Learning agent with experience replay memory and Random Network Distillation (RND).\n",
    "Two deep convolutional Q networks (DCQNs) are used for stability: the primary, and the target.\n",
    "The target network is updated to have the same weights as the primary one every *SAVE_EVERY* frames.\n",
    "Each state consists of the sensory (image) observation and its three preceding observations.\n",
    "The DCQN performs multiple convolutions on each of these states for visual pattern learning.\n",
    "RND is used to give the agent reward for discovering new states (thus exploring the levels).\n",
    "The fully connected layers (after convolutions) are split into dueling layers which are then re-combined using mean advantage.\n",
    "Finally, for a slight improvement in training speed, new experiences are sent to and stored in the replay buffer on the GPU.\n",
    "\n",
    "### Performance on other Games\n",
    "With slightly different hyperparameter values (and using QNetwork instead of DuelingConvQNetwork), the agent is able to\n",
    "play Cart-Pole to an average score of 440 after approximately 600 episodes.\n",
    "\n",
    "### References\n",
    "#### Code\n",
    "- https://github.com/seungeunrho/minimalRL/blob/master/dqn.py, released under the MIT licence.\n",
    "- https://github.com/astooke/rlpyt, released under the MIT licence.\n",
    "- https://github.com/simoninithomas/Deep_reinforcement_learning_Course/tree/master/RND%20Montezuma's%20revenge%20PyTorch,\n",
    "released under the MIT licence.\n",
    "- https://github.com/noagarcia/visdom-tutorial/blob/master/utils.py, licence not provided, presented as open-source tutorial.\n",
    "\n",
    "#### Papers\n",
    "- Burda, Y., Edwards, H., Storkey, A. and Klimov, O., 2018. Exploration by random network distillation. arXiv preprint\n",
    "arXiv:1810.12894.\n",
    "- Schaul, T., Quan, J., Antonoglou, I. and Silver, D., 2015. Prioritized experience replay. arXiv preprint\n",
    "arXiv:1511.05952.\n",
    "- Hasselt, H., 2010. Double Q-learning. Advances in neural information processing systems, 23, pp.2613-2621.\n",
    "- Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.A., Veness, J., Bellemare, M.G., Graves, A., Riedmiller, M., Fidjeland,\n",
    "A.K., Ostrovski, G. and Petersen, S., 2015. Human-level control through deep reinforcement learning.\n",
    "nature, 518(7540), pp.529-533.\n",
    "- Wang, Z., Schaul, T., Hessel, M., Hasselt, H., Lanctot, M. and Freitas, N., 2016, June. Dueling network architectures\n",
    "for deep reinforcement learning. In International conference on machine learning (pp. 1995-2003). PMLR.\n",
    "- Parr, B., 2018. Deep In-GPU Experience Replay. arXiv preprint arXiv:1801.03138.\n",
    "\n",
    "### Requirements\n",
    "\n",
    "- OpenAI Gym with Atari Environments\n",
    "- Numpy\n",
    "- PyTorch\n",
    "- Visdom for stat visualisation\n",
    "- line_profiler for profiling\n",
    "\n",
    "---\n",
    "\n",
    "### Imports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# %load_ext line_profiler\n",
    "\n",
    "import collections\n",
    "import gym\n",
    "from gym.wrappers import Monitor, AtariPreprocessing\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from visdom import Visdom"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "### Configuration and Hyperparameters"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "LEARNING_RATE = 1e-4\n",
    "GAMMA = 0.99\n",
    "\n",
    "EPSILON_MAX = 0.50\n",
    "EPSILON_MIN = 0.02\n",
    "EPSILON_REDUCTION = 1.0 / 2000\n",
    "\n",
    "SAVE_EVERY = 100\n",
    "STEPS_PER_TRAIN = 10\n",
    "\n",
    "BUFFER_SIZE_MAX = 50000\n",
    "BUFFER_SIZE_MIN = 2000\n",
    "BUFFER_BATCH_SIZE = 128\n",
    "\n",
    "INTRINSIC_CLIP = 5.0\n",
    "INTRINSIC_SAMPLE_SIZE = 1000\n",
    "UPDATE_PROPORTION = 0.25\n",
    "INTRINSIC_REWARD_FACTOR = 0.5\n",
    "\n",
    "EX_SCORE_NORM = 1 / 500  # Extrinsic reward normalising factor\n",
    "IN_SCORE_NORM = 1 / 500  # Intrinsic reward normalising factor\n",
    "\n",
    "# Configuration\n",
    "ENVIRONMENT = \"Gravitar-v0\"\n",
    "# ENVIRONMENT = \"Breakout-v0\"\n",
    "# ENVIRONMENT = \"Gravitar-ram-v0\"\n",
    "# ENVIRONMENT = \"CartPole-v1\"\n",
    "\n",
    "ATARI_IMAGE_ENV = ENVIRONMENT in (\"Gravitar-v0\", \"Breakout-v0\")\n",
    "\n",
    "STATE_VALUE_FACTOR = (1.0 / 255.0) if ATARI_IMAGE_ENV else 1.0\n",
    "STATE_STORAGE_TYPE = torch.uint8 if ATARI_IMAGE_ENV else torch.float\n",
    "\n",
    "VIDEO_EVERY = 25\n",
    "OUTPUT_EVERY = 5\n",
    "MEAN_EVERY = 100\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "### Utilities and Helpers"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n"
     ]
    }
   ],
   "source": [
    "class VisdomLinePlotter:\n",
    "    \"\"\"Plots to Visdom\"\"\"\n",
    "    def __init__(self, env_name=\"main\", label_x=\"epochs\"):\n",
    "        self.vis = Visdom()\n",
    "        self.env = env_name\n",
    "        self.plots = {}\n",
    "        self.label_x = label_x\n",
    "\n",
    "    def clear_envs(self):\n",
    "        for env in self.vis.get_env_list():\n",
    "            self.vis.delete_env(env)\n",
    "\n",
    "    def plot(self, var_name, split_name, title_name, x, y):\n",
    "        if var_name not in self.plots:\n",
    "            self.plots[var_name] = self.vis.line(\n",
    "                X=np.array([x, x]), Y=np.array([y, y]),\n",
    "                env=self.env,\n",
    "                opts=dict(\n",
    "                    legend=[split_name],\n",
    "                    title=title_name,\n",
    "                    xlabel=self.label_x,\n",
    "                    ylabel=var_name\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            self.vis.line(\n",
    "                X=np.array([x]), Y=np.array([y]),\n",
    "                env=self.env,\n",
    "                win=self.plots[var_name],\n",
    "                name=split_name,\n",
    "                update = \"append\"\n",
    "            )\n",
    "\n",
    "\n",
    "line_plotter = VisdomLinePlotter(label_x=\"Episodes\")\n",
    "line_plotter.clear_envs()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "### Reinforcement Learning Model\n",
    "\n",
    "Includes Experience Replay Buffer, Deep-Q model and Agent model."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "    def __init__(self, observation_shape, size_limit: int, min_memory: int, batch_size: int, device):\n",
    "        self.size_limit: int = size_limit\n",
    "        self.min_memory: int = min_memory\n",
    "        self.batch_size: int = batch_size\n",
    "\n",
    "        self.count: int = 0\n",
    "\n",
    "        self.states         = torch.zeros((size_limit, *observation_shape), device=device, dtype=STATE_STORAGE_TYPE)\n",
    "        self.actions        = torch.zeros((size_limit, 1), device=device, dtype=torch.long)\n",
    "        self.rewards        = torch.zeros((size_limit, 1), device=device, dtype=torch.float)\n",
    "        self.states_        = torch.zeros((size_limit, *observation_shape), device=device, dtype=STATE_STORAGE_TYPE)\n",
    "        self.terminal_masks = torch.zeros((size_limit, 1), device=device, dtype=torch.float)\n",
    "        self.priorities     = torch.ones((size_limit,))\n",
    "\n",
    "    def put(self, state, action, reward, state_, terminal_mask: float):\n",
    "        idx = self.count % self.size_limit\n",
    "\n",
    "        self.states[idx]         = state\n",
    "        self.actions[idx]        = action\n",
    "        self.rewards[idx]        = reward\n",
    "        self.states_[idx]        = state_\n",
    "        self.terminal_masks[idx] = terminal_mask\n",
    "\n",
    "        # self.buffer_p[self.pos] = max(self.buffer_p[:len(self)], default=1)\n",
    "\n",
    "        self.count += 1\n",
    "\n",
    "    def sample(self):\n",
    "        probabilities = self.priorities[:len(self)]\n",
    "        sampled_idx = probabilities.multinomial(num_samples=self.batch_size, replacement=True)\n",
    "\n",
    "        return \\\n",
    "            self.states[sampled_idx].float(), \\\n",
    "            self.actions[sampled_idx], \\\n",
    "            self.rewards[sampled_idx], \\\n",
    "            self.states_[sampled_idx].float(), \\\n",
    "            self.terminal_masks[sampled_idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return min(self.count, self.size_limit)\n",
    "\n",
    "    def can_sample(self):\n",
    "        return len(self) > self.min_memory\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_shape, n_actions):\n",
    "        super().__init__()\n",
    "\n",
    "        obs_length = np.array(state_shape).prod()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(obs_length, 128), nn.ReLU(),\n",
    "            nn.Linear(128, 256), nn.ReLU(),\n",
    "            nn.Linear(256, 128), nn.ReLU(),\n",
    "            nn.Linear(128, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x * STATE_VALUE_FACTOR)\n",
    "\n",
    "def conv_block(in_channels):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, 32, (8, 8), (4, 4)), nn.ReLU(),\n",
    "        nn.Conv2d(32, 64, (4, 4), (2, 2)), nn.ReLU(),\n",
    "        nn.Conv2d(64, 64, (3, 3), (1, 1)), nn.ReLU()\n",
    "    )\n",
    "\n",
    "\n",
    "class DuelingConvQNetwork(nn.Module):\n",
    "    def __init__(self, n_actions):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = conv_block(4)\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(64 * 7 * 7, 512), nn.ReLU(),\n",
    "            nn.Linear(512, 512), nn.ReLU(),\n",
    "            nn.Linear(512, n_actions)\n",
    "        )\n",
    "\n",
    "        self.fc_dueling = nn.Sequential(\n",
    "            nn.Linear(64 * 7 * 7, 512), nn.ReLU(),\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x * STATE_VALUE_FACTOR)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        advantages = self.fc(x)\n",
    "        value = self.fc_dueling(x)\n",
    "        return value + advantages - advantages.mean(dim=-1, keepdim=True)\n",
    "\n",
    "\n",
    "class IntrinsicNetwork(nn.Module):\n",
    "    def __init__(self, obs_shape, is_target):\n",
    "        super().__init__()\n",
    "\n",
    "        if ATARI_IMAGE_ENV:\n",
    "            self.conv = conv_block(4)\n",
    "            in_features = 64 * 7 * 7\n",
    "        else:\n",
    "            in_features = np.array(obs_shape).prod()\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(in_features, 512), nn.ReLU(),\n",
    "            # Add an additional two fully connected layers if this is the non-target\n",
    "            *[block for _ in range(0 if is_target else 2) for block in [nn.ReLU(), nn.Linear(512, 512)]]\n",
    "        )\n",
    "\n",
    "        for p in self.modules():\n",
    "            if isinstance(p, (nn.Conv2d, nn.Linear)):\n",
    "                torch.nn.init.orthogonal_(p.weight, np.sqrt(2))\n",
    "                p.bias.data.zero_()\n",
    "\n",
    "        # Disable training for target network\n",
    "        if is_target:\n",
    "            for p in self.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        if ATARI_IMAGE_ENV:\n",
    "            x = self.conv(x)\n",
    "        return self.fc(x.view(x.shape[0], -1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    def __init__(\n",
    "            self, observation_space, action_space, device,\n",
    "            buffer_size, min_buffer_size, batch_size,\n",
    "            update_proportion, intrinsic_reward_factor):\n",
    "        self.observation_space = observation_space\n",
    "        self.action_space = action_space\n",
    "        self.device = device\n",
    "        self.intrinsic_reward_factor = intrinsic_reward_factor\n",
    "\n",
    "        state_shape = (4, 84, 84) if ATARI_IMAGE_ENV else observation_space.shape\n",
    "\n",
    "        self.replay_buffer: ReplayBuffer = ReplayBuffer(\n",
    "            state_shape, buffer_size, min_buffer_size, batch_size, DEVICE)\n",
    "\n",
    "        self.q_net: nn.Module = \\\n",
    "            (DuelingConvQNetwork(action_space.n) if ATARI_IMAGE_ENV\n",
    "             else QNetwork(state_shape, action_space.n)).to(device)\n",
    "        self.q_target: nn.Module = \\\n",
    "            (DuelingConvQNetwork(action_space.n) if ATARI_IMAGE_ENV\n",
    "             else QNetwork(state_shape, action_space.n)).to(device)\n",
    "        self.q_target.load_state_dict(self.q_net.state_dict())\n",
    "\n",
    "        self.f_predictor = IntrinsicNetwork(state_shape, False).to(device)\n",
    "        self.f_target = IntrinsicNetwork(state_shape, True).to(device)\n",
    "        self.mse_loss = nn.MSELoss(reduction=\"none\")\n",
    "        self.update_proportion = update_proportion\n",
    "\n",
    "        self.optimiser = \\\n",
    "            optim.Adam(list(self.q_net.parameters()) + list(self.f_predictor.parameters()), lr=LEARNING_RATE)\n",
    "\n",
    "    def remember(self, state, action, reward, state_, done: bool):\n",
    "        self.replay_buffer.put(state, action, reward, state_, 0.0 if done else 1.0)\n",
    "\n",
    "    def train_step(self, n_episode):\n",
    "        if not self.replay_buffer.can_sample():\n",
    "            return\n",
    "\n",
    "        last_rnd_loss = 0.0\n",
    "        last_loss = 0.0\n",
    "        last_expected_reward = 0.0\n",
    "\n",
    "        for i in range(STEPS_PER_TRAIN):\n",
    "            states, actions, rewards, states_, done_masks = self.replay_buffer.sample()\n",
    "\n",
    "            target_next_feature, predicted_next_feature = self.rnd(states_)\n",
    "\n",
    "            # --------------- RND\n",
    "            rnd_loss = self.mse_loss(predicted_next_feature, target_next_feature.detach()).mean(-1)\n",
    "            mask = torch.rand(len(rnd_loss)).to(self.device)\n",
    "            mask = mask.__lt__(self.update_proportion).type(torch.FloatTensor).to(self.device)\n",
    "            rnd_loss = (rnd_loss * mask).sum() / torch.max(mask.sum(), torch.Tensor([1]).to(self.device))\n",
    "\n",
    "            if i == STEPS_PER_TRAIN - 1:\n",
    "                last_rnd_loss = rnd_loss.detach().item()\n",
    "            # ---------------\n",
    "\n",
    "            q_out = self.q_net(states)\n",
    "            q_sa = q_out.gather(1, actions)\n",
    "\n",
    "            max_q_prime = self.q_target(states_).max(1)[0].unsqueeze(1)\n",
    "            target = rewards + GAMMA * max_q_prime * done_masks\n",
    "\n",
    "            # print(\"FL:\", forward_loss)\n",
    "            loss = F.smooth_l1_loss(q_sa, target.detach()) + rnd_loss.detach()\n",
    "\n",
    "            if i == STEPS_PER_TRAIN - 1:\n",
    "                last_loss = loss.detach().item()\n",
    "                last_expected_reward = q_sa.detach().mean().item()\n",
    "\n",
    "            self.optimiser.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimiser.step()\n",
    "\n",
    "        # print(last_loss, last_expected_reward)\n",
    "        line_plotter.plot(\"Loss\", \"val\", \"RND Loss\", n_episode, last_rnd_loss)\n",
    "        line_plotter.plot(\"Loss\", \"val\", \"Network Loss\", n_episode, last_loss)\n",
    "        line_plotter.plot(\"Expected Reward\", \"reward\", \"Expected Return\", n_episode, last_expected_reward)\n",
    "\n",
    "    def rematch_networks(self):\n",
    "        self.q_target.load_state_dict(self.q_net.state_dict())\n",
    "\n",
    "    def sample_action(self, state, epsilon):\n",
    "        # Exploitation vs Exploration\n",
    "        return \\\n",
    "            self.q_net.forward(state).argmax().item() if random.random() > epsilon else self.action_space.sample()\n",
    "\n",
    "    def rnd(self, state_):\n",
    "        target_next_feature = self.f_target(state_ * STATE_VALUE_FACTOR)\n",
    "        predicted_next_feature = self.f_predictor(state_)\n",
    "        return target_next_feature, predicted_next_feature\n",
    "\n",
    "    def intrinsic_reward(self, state_):\n",
    "        target_next_feature, predicted_next_feature = self.rnd(state_)\n",
    "        intrinsic_reward = (target_next_feature - predicted_next_feature).pow(2).sum(-1) * self.intrinsic_reward_factor\n",
    "        return intrinsic_reward.cpu().detach().item()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "### Training Loop"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def main():\n",
    "    with gym.make(ENVIRONMENT) as env:\n",
    "        if ATARI_IMAGE_ENV:\n",
    "            env = AtariPreprocessing(env, frame_skip=1)  # Gravitar-v0 already skips frames\n",
    "\n",
    "        env = Monitor(\n",
    "            env,\n",
    "            \"./video\",\n",
    "            video_callable=lambda episode_id: not episode_id % VIDEO_EVERY and episode_id,\n",
    "            force=True\n",
    "        )\n",
    "\n",
    "        #region Random seed initialisation for reproducible environment - do not change\n",
    "        seed = 742\n",
    "        torch.manual_seed(seed)\n",
    "        env.seed(seed)\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        env.action_space.seed(seed)\n",
    "        #endregion\n",
    "\n",
    "        agent: Agent = Agent(\n",
    "            env.observation_space, env.action_space, DEVICE,\n",
    "            BUFFER_SIZE_MAX, BUFFER_SIZE_MIN, BUFFER_BATCH_SIZE,\n",
    "            UPDATE_PROPORTION, INTRINSIC_REWARD_FACTOR\n",
    "        )\n",
    "\n",
    "        marking = []\n",
    "        score_history =  collections.deque(maxlen=MEAN_EVERY)\n",
    "        normalised_intrinsic =  collections.deque(maxlen=INTRINSIC_SAMPLE_SIZE)\n",
    "\n",
    "        line_plotter.plot(\"Score\", \"max\", \"Agent Score\", 0.0, 0.0)\n",
    "        line_plotter.plot( \"Score\", \"min\", \"Agent Score\", 0.0, 0.0)\n",
    "        line_plotter.plot(\"Score\", \"val\", \"Agent Score\", 0.0, 0.0)\n",
    "        line_plotter.plot(\"Score\", \"average\", \"Agent Score\", 0.0, 0.0)\n",
    "\n",
    "        best_v_score = 0\n",
    "        best_v_ep = 0\n",
    "\n",
    "        intrinsic_rewards =  collections.deque(maxlen=INTRINSIC_SAMPLE_SIZE)\n",
    "\n",
    "        for n_episode in range(int(1e32)):\n",
    "            epsilon = max(EPSILON_MIN, EPSILON_MAX - n_episode * EPSILON_REDUCTION)\n",
    "            score = 0.0\n",
    "\n",
    "            if ATARI_IMAGE_ENV:\n",
    "                state = torch.cat([torch.as_tensor(env.reset(), dtype=torch.uint8, device=DEVICE).unsqueeze(0)] * 4)\n",
    "            else:\n",
    "                state = torch.as_tensor(env.reset(), dtype=torch.float, device=DEVICE)\n",
    "\n",
    "            while True:\n",
    "                # Sample an action from the agent and use it\n",
    "                chosen_action = agent.sample_action(state.unsqueeze(0).float(), epsilon)\n",
    "                obs_, extrinsic_reward, done, info = env.step(chosen_action)\n",
    "\n",
    "                # Convert observation to state\n",
    "                if ATARI_IMAGE_ENV:\n",
    "                    state_ = torch.cat(\n",
    "                        (state[1:], torch.as_tensor(obs_, dtype=torch.uint8, device=DEVICE).unsqueeze(0)))\n",
    "                else:\n",
    "                    state_ = torch.as_tensor(obs_, dtype=torch.float, device=DEVICE)\n",
    "\n",
    "                # RND intrinsic reward calculation\n",
    "                intrinsic_reward = agent.intrinsic_reward(state_.unsqueeze(0).float())\n",
    "                intrinsic_rewards.append(intrinsic_reward)\n",
    "                std = np.std(intrinsic_rewards)\n",
    "                if std != 0.0:\n",
    "                    intrinsic_reward = \\\n",
    "                        ((intrinsic_reward - np.mean(intrinsic_rewards)) / std) \\\n",
    "                        .clip(-INTRINSIC_CLIP, INTRINSIC_CLIP)\n",
    "                    normalised_intrinsic.append(intrinsic_reward)\n",
    "\n",
    "                # Combine intrinsic and extrinsic rewards\n",
    "                reward = intrinsic_reward * IN_SCORE_NORM + extrinsic_reward * EX_SCORE_NORM\n",
    "\n",
    "                # Store the experience\n",
    "                agent.remember(state, chosen_action, reward, state_, done)\n",
    "\n",
    "                state = state_\n",
    "\n",
    "                score += extrinsic_reward\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            score_history.append(score)\n",
    "\n",
    "            agent.train_step(n_episode)\n",
    "\n",
    "            if not (n_episode % SAVE_EVERY) and n_episode:\n",
    "                print(\"Matching network values\")\n",
    "                agent.rematch_networks()\n",
    "\n",
    "            if n_episode:\n",
    "                line_plotter.plot(\"Score\", \"val\", \"Agent Score\", n_episode, score)\n",
    "                line_plotter.plot(\"Reward\", \"val\", \"Average Intrinsic Reward\", n_episode, np.mean(normalised_intrinsic))\n",
    "                line_plotter.plot(\"Epsilon\", \"val\", \"Epsilon\", n_episode, epsilon)\n",
    "\n",
    "                if not n_episode % MEAN_EVERY:\n",
    "                    line_plotter.plot(\"Score\", \"average\", \"Agent Score\", n_episode, np.array(score_history).mean())\n",
    "                    max_score = np.array(score_history).max(initial=0)\n",
    "                    line_plotter.plot(\"Score\", \"max\", \"Agent Score\", n_episode, max_score)\n",
    "                    line_plotter.plot(\n",
    "                        \"Score\", \"min\", \"Agent Score\", n_episode, np.array(score_history).min(initial=max_score))\n",
    "\n",
    "                if not n_episode % VIDEO_EVERY:\n",
    "                    if score > best_v_score:\n",
    "                        best_v_score = score\n",
    "                        best_v_ep = n_episode\n",
    "                    print(f\"video: {n_episode}, score: {score:.0f} (best {best_v_score} at episode {best_v_ep})\")\n",
    "                if not n_episode % OUTPUT_EVERY:\n",
    "                    print(f\"episode: {n_episode}, \"\n",
    "                          f\"score: {score:.0f}, \"\n",
    "                          f\"epsilon: {epsilon:.3f}\" +\n",
    "                          (f\", buffer size: {len(agent.replay_buffer)}\"\n",
    "                           if len(agent.replay_buffer) != BUFFER_SIZE_MAX else \"\"))\n",
    "                          # (f\", loss: {mean_loss:.2g}\" if mean_loss is not None else \"\"))\n",
    "\n",
    "            #region Submission log marking - do not change\n",
    "            marking.append(score)\n",
    "            if n_episode % 100 == 0:\n",
    "                print(\"marking, episode: {}, score: {:.1f}, mean_score: {:.2f}, std_score: {:.2f}\".format(\n",
    "                    n_episode, score, np.array(marking).mean(), np.array(marking).std()))\n",
    "                marking = []\n",
    "            #endregion"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "marking, episode: 0, score: 0.0, mean_score: 0.00, std_score: 0.00\n",
      "episode: 5, score: 500, epsilon: 0.497, buffer size: 5156\n",
      "episode: 10, score: 250, epsilon: 0.495, buffer size: 9122\n",
      "episode: 15, score: 0, epsilon: 0.492, buffer size: 13876\n",
      "episode: 20, score: 100, epsilon: 0.490, buffer size: 18807\n",
      "video: 25, score: 700 (best 700.0 at episode 25)\n",
      "episode: 25, score: 700, epsilon: 0.487, buffer size: 23248\n",
      "episode: 30, score: 0, epsilon: 0.485, buffer size: 28278\n",
      "episode: 35, score: 100, epsilon: 0.482, buffer size: 32323\n",
      "episode: 40, score: 0, epsilon: 0.480, buffer size: 36880\n",
      "episode: 45, score: 200, epsilon: 0.477, buffer size: 42170\n",
      "video: 50, score: 0 (best 700.0 at episode 25)\n",
      "episode: 50, score: 0, epsilon: 0.475, buffer size: 46389\n",
      "episode: 55, score: 0, epsilon: 0.472\n",
      "episode: 60, score: 0, epsilon: 0.470\n",
      "episode: 65, score: 0, epsilon: 0.468\n",
      "episode: 70, score: 0, epsilon: 0.465\n",
      "video: 75, score: 0 (best 700.0 at episode 25)\n",
      "episode: 75, score: 0, epsilon: 0.463\n",
      "episode: 80, score: 0, epsilon: 0.460\n",
      "episode: 85, score: 0, epsilon: 0.458\n",
      "episode: 90, score: 700, epsilon: 0.455\n",
      "episode: 95, score: 500, epsilon: 0.453\n",
      "Matching network values\n",
      "video: 100, score: 100 (best 700.0 at episode 25)\n",
      "episode: 100, score: 100, epsilon: 0.450\n",
      "marking, episode: 100, score: 100.0, mean_score: 148.50, std_score: 201.67\n",
      "episode: 105, score: 100, epsilon: 0.448\n",
      "episode: 110, score: 250, epsilon: 0.445\n",
      "episode: 115, score: 0, epsilon: 0.443\n",
      "episode: 120, score: 200, epsilon: 0.440\n",
      "video: 125, score: 100 (best 700.0 at episode 25)\n",
      "episode: 125, score: 100, epsilon: 0.438\n",
      "episode: 130, score: 100, epsilon: 0.435\n",
      "episode: 135, score: 200, epsilon: 0.432\n",
      "episode: 140, score: 750, epsilon: 0.430\n",
      "episode: 145, score: 100, epsilon: 0.427\n",
      "video: 150, score: 250 (best 700.0 at episode 25)\n",
      "episode: 150, score: 250, epsilon: 0.425\n",
      "episode: 155, score: 500, epsilon: 0.422\n",
      "episode: 160, score: 0, epsilon: 0.420\n",
      "episode: 165, score: 0, epsilon: 0.417\n",
      "episode: 170, score: 500, epsilon: 0.415\n",
      "video: 175, score: 100 (best 700.0 at episode 25)\n",
      "episode: 175, score: 100, epsilon: 0.412\n",
      "episode: 180, score: 350, epsilon: 0.410\n",
      "episode: 185, score: 0, epsilon: 0.407\n",
      "episode: 190, score: 0, epsilon: 0.405\n",
      "episode: 195, score: 0, epsilon: 0.402\n",
      "Matching network values\n",
      "video: 200, score: 0 (best 700.0 at episode 25)\n",
      "episode: 200, score: 0, epsilon: 0.400\n",
      "marking, episode: 200, score: 0.0, mean_score: 139.00, std_score: 169.50\n",
      "episode: 205, score: 700, epsilon: 0.397\n",
      "episode: 210, score: 0, epsilon: 0.395\n",
      "episode: 215, score: 0, epsilon: 0.393\n",
      "episode: 220, score: 0, epsilon: 0.390\n",
      "video: 225, score: 0 (best 700.0 at episode 25)\n",
      "episode: 225, score: 0, epsilon: 0.388\n",
      "episode: 230, score: 0, epsilon: 0.385\n",
      "episode: 235, score: 200, epsilon: 0.383\n",
      "episode: 240, score: 250, epsilon: 0.380\n",
      "episode: 245, score: 100, epsilon: 0.378\n",
      "video: 250, score: 0 (best 700.0 at episode 25)\n",
      "episode: 250, score: 0, epsilon: 0.375\n",
      "episode: 255, score: 600, epsilon: 0.372\n",
      "episode: 260, score: 0, epsilon: 0.370\n",
      "episode: 265, score: 300, epsilon: 0.367\n",
      "episode: 270, score: 0, epsilon: 0.365\n",
      "video: 275, score: 600 (best 700.0 at episode 25)\n",
      "episode: 275, score: 600, epsilon: 0.362\n",
      "episode: 280, score: 0, epsilon: 0.360\n",
      "episode: 285, score: 0, epsilon: 0.357\n",
      "episode: 290, score: 250, epsilon: 0.355\n",
      "episode: 295, score: 0, epsilon: 0.353\n",
      "Matching network values\n",
      "video: 300, score: 0 (best 700.0 at episode 25)\n",
      "episode: 300, score: 0, epsilon: 0.350\n",
      "marking, episode: 300, score: 0.0, mean_score: 96.00, std_score: 150.28\n",
      "episode: 305, score: 100, epsilon: 0.348\n",
      "episode: 310, score: 0, epsilon: 0.345\n",
      "episode: 315, score: 100, epsilon: 0.343\n",
      "episode: 320, score: 0, epsilon: 0.340\n",
      "video: 325, score: 0 (best 700.0 at episode 25)\n",
      "episode: 325, score: 0, epsilon: 0.338\n",
      "episode: 330, score: 200, epsilon: 0.335\n",
      "episode: 335, score: 0, epsilon: 0.333\n",
      "episode: 340, score: 0, epsilon: 0.330\n",
      "episode: 345, score: 100, epsilon: 0.328\n",
      "video: 350, score: 0 (best 700.0 at episode 25)\n",
      "episode: 350, score: 0, epsilon: 0.325\n",
      "episode: 355, score: 100, epsilon: 0.323\n",
      "episode: 360, score: 0, epsilon: 0.320\n",
      "episode: 365, score: 0, epsilon: 0.318\n",
      "episode: 370, score: 0, epsilon: 0.315\n",
      "video: 375, score: 0 (best 700.0 at episode 25)\n",
      "episode: 375, score: 0, epsilon: 0.312\n",
      "episode: 380, score: 0, epsilon: 0.310\n",
      "episode: 385, score: 500, epsilon: 0.307\n",
      "episode: 390, score: 600, epsilon: 0.305\n",
      "episode: 395, score: 500, epsilon: 0.302\n",
      "Matching network values\n",
      "video: 400, score: 0 (best 700.0 at episode 25)\n",
      "episode: 400, score: 0, epsilon: 0.300\n",
      "marking, episode: 400, score: 0.0, mean_score: 101.50, std_score: 176.70\n",
      "episode: 405, score: 0, epsilon: 0.297\n",
      "episode: 410, score: 500, epsilon: 0.295\n",
      "episode: 415, score: 200, epsilon: 0.292\n",
      "episode: 420, score: 0, epsilon: 0.290\n",
      "video: 425, score: 350 (best 700.0 at episode 25)\n",
      "episode: 425, score: 350, epsilon: 0.287\n",
      "episode: 430, score: 0, epsilon: 0.285\n",
      "episode: 435, score: 0, epsilon: 0.282\n",
      "episode: 440, score: 0, epsilon: 0.280\n",
      "episode: 445, score: 100, epsilon: 0.277\n",
      "video: 450, score: 100 (best 700.0 at episode 25)\n",
      "episode: 450, score: 100, epsilon: 0.275\n",
      "episode: 455, score: 250, epsilon: 0.272\n",
      "episode: 460, score: 100, epsilon: 0.270\n",
      "episode: 465, score: 0, epsilon: 0.267\n",
      "episode: 470, score: 0, epsilon: 0.265\n",
      "video: 475, score: 0 (best 700.0 at episode 25)\n",
      "episode: 475, score: 0, epsilon: 0.262\n",
      "episode: 480, score: 250, epsilon: 0.260\n",
      "episode: 485, score: 0, epsilon: 0.258\n",
      "episode: 490, score: 0, epsilon: 0.255\n",
      "episode: 495, score: 100, epsilon: 0.253\n",
      "Matching network values\n",
      "video: 500, score: 200 (best 700.0 at episode 25)\n",
      "episode: 500, score: 200, epsilon: 0.250\n",
      "marking, episode: 500, score: 200.0, mean_score: 93.00, std_score: 160.16\n",
      "episode: 505, score: 100, epsilon: 0.247\n",
      "episode: 510, score: 0, epsilon: 0.245\n",
      "episode: 515, score: 0, epsilon: 0.242\n",
      "episode: 520, score: 0, epsilon: 0.240\n",
      "video: 525, score: 0 (best 700.0 at episode 25)\n",
      "episode: 525, score: 0, epsilon: 0.237\n",
      "episode: 530, score: 0, epsilon: 0.235\n",
      "episode: 535, score: 0, epsilon: 0.232\n",
      "episode: 540, score: 0, epsilon: 0.230\n",
      "episode: 545, score: 0, epsilon: 0.227\n",
      "video: 550, score: 0 (best 700.0 at episode 25)\n",
      "episode: 550, score: 0, epsilon: 0.225\n",
      "episode: 555, score: 0, epsilon: 0.222\n",
      "episode: 560, score: 100, epsilon: 0.220\n",
      "episode: 565, score: 0, epsilon: 0.217\n",
      "episode: 570, score: 0, epsilon: 0.215\n",
      "video: 575, score: 250 (best 700.0 at episode 25)\n",
      "episode: 575, score: 250, epsilon: 0.212\n",
      "episode: 580, score: 0, epsilon: 0.210\n",
      "episode: 585, score: 250, epsilon: 0.208\n",
      "episode: 590, score: 450, epsilon: 0.205\n",
      "episode: 595, score: 250, epsilon: 0.203\n",
      "Matching network values\n",
      "video: 600, score: 0 (best 700.0 at episode 25)\n",
      "episode: 600, score: 0, epsilon: 0.200\n",
      "marking, episode: 600, score: 0.0, mean_score: 90.00, std_score: 173.06\n",
      "episode: 605, score: 0, epsilon: 0.198\n",
      "episode: 610, score: 100, epsilon: 0.195\n",
      "episode: 615, score: 200, epsilon: 0.193\n",
      "episode: 620, score: 100, epsilon: 0.190\n",
      "video: 625, score: 0 (best 700.0 at episode 25)\n",
      "episode: 625, score: 0, epsilon: 0.188\n",
      "episode: 630, score: 200, epsilon: 0.185\n",
      "episode: 635, score: 100, epsilon: 0.182\n",
      "episode: 640, score: 0, epsilon: 0.180\n",
      "episode: 645, score: 0, epsilon: 0.177\n",
      "video: 650, score: 0 (best 700.0 at episode 25)\n",
      "episode: 650, score: 0, epsilon: 0.175\n",
      "episode: 655, score: 250, epsilon: 0.172\n",
      "episode: 660, score: 0, epsilon: 0.170\n",
      "episode: 665, score: 0, epsilon: 0.167\n",
      "episode: 670, score: 0, epsilon: 0.165\n",
      "video: 675, score: 300 (best 700.0 at episode 25)\n",
      "episode: 675, score: 300, epsilon: 0.162\n",
      "episode: 680, score: 0, epsilon: 0.160\n",
      "episode: 685, score: 400, epsilon: 0.157\n",
      "episode: 690, score: 0, epsilon: 0.155\n",
      "episode: 695, score: 0, epsilon: 0.152\n",
      "Matching network values\n",
      "video: 700, score: 0 (best 700.0 at episode 25)\n",
      "episode: 700, score: 0, epsilon: 0.150\n",
      "marking, episode: 700, score: 0.0, mean_score: 97.00, std_score: 147.96\n",
      "episode: 705, score: 0, epsilon: 0.148\n",
      "episode: 710, score: 0, epsilon: 0.145\n",
      "episode: 715, score: 0, epsilon: 0.143\n",
      "episode: 720, score: 0, epsilon: 0.140\n",
      "video: 725, score: 0 (best 700.0 at episode 25)\n",
      "episode: 725, score: 0, epsilon: 0.138\n",
      "episode: 730, score: 0, epsilon: 0.135\n",
      "episode: 735, score: 100, epsilon: 0.133\n",
      "episode: 740, score: 0, epsilon: 0.130\n",
      "episode: 745, score: 450, epsilon: 0.128\n",
      "video: 750, score: 350 (best 700.0 at episode 25)\n",
      "episode: 750, score: 350, epsilon: 0.125\n",
      "episode: 755, score: 0, epsilon: 0.122\n",
      "episode: 760, score: 100, epsilon: 0.120\n",
      "episode: 765, score: 100, epsilon: 0.117\n",
      "episode: 770, score: 0, epsilon: 0.115\n",
      "video: 775, score: 0 (best 700.0 at episode 25)\n",
      "episode: 775, score: 0, epsilon: 0.112\n",
      "episode: 780, score: 300, epsilon: 0.110\n",
      "episode: 785, score: 0, epsilon: 0.107\n",
      "episode: 790, score: 100, epsilon: 0.105\n",
      "episode: 795, score: 0, epsilon: 0.102\n",
      "Matching network values\n",
      "video: 800, score: 0 (best 700.0 at episode 25)\n",
      "episode: 800, score: 0, epsilon: 0.100\n",
      "marking, episode: 800, score: 0.0, mean_score: 53.00, std_score: 107.20\n",
      "episode: 805, score: 0, epsilon: 0.097\n",
      "episode: 810, score: 500, epsilon: 0.095\n",
      "episode: 815, score: 0, epsilon: 0.092\n",
      "episode: 820, score: 0, epsilon: 0.090\n",
      "video: 825, score: 100 (best 700.0 at episode 25)\n",
      "episode: 825, score: 100, epsilon: 0.087\n",
      "episode: 830, score: 0, epsilon: 0.085\n",
      "episode: 835, score: 0, epsilon: 0.083\n",
      "episode: 840, score: 250, epsilon: 0.080\n",
      "episode: 845, score: 0, epsilon: 0.078\n",
      "video: 850, score: 0 (best 700.0 at episode 25)\n",
      "episode: 850, score: 0, epsilon: 0.075\n",
      "episode: 855, score: 0, epsilon: 0.073\n",
      "episode: 860, score: 0, epsilon: 0.070\n",
      "episode: 865, score: 250, epsilon: 0.068\n",
      "episode: 870, score: 0, epsilon: 0.065\n",
      "video: 875, score: 0 (best 700.0 at episode 25)\n",
      "episode: 875, score: 0, epsilon: 0.062\n",
      "episode: 880, score: 0, epsilon: 0.060\n",
      "episode: 885, score: 0, epsilon: 0.057\n",
      "episode: 890, score: 0, epsilon: 0.055\n",
      "episode: 895, score: 250, epsilon: 0.052\n",
      "Matching network values\n",
      "video: 900, score: 0 (best 700.0 at episode 25)\n",
      "episode: 900, score: 0, epsilon: 0.050\n",
      "marking, episode: 900, score: 0.0, mean_score: 61.00, std_score: 115.02\n",
      "episode: 905, score: 250, epsilon: 0.047\n",
      "episode: 910, score: 0, epsilon: 0.045\n",
      "episode: 915, score: 0, epsilon: 0.042\n",
      "episode: 920, score: 0, epsilon: 0.040\n",
      "video: 925, score: 0 (best 700.0 at episode 25)\n",
      "episode: 925, score: 0, epsilon: 0.037\n",
      "episode: 930, score: 100, epsilon: 0.035\n",
      "episode: 935, score: 100, epsilon: 0.032\n",
      "episode: 940, score: 0, epsilon: 0.030\n",
      "episode: 945, score: 0, epsilon: 0.027\n",
      "video: 950, score: 0 (best 700.0 at episode 25)\n",
      "episode: 950, score: 0, epsilon: 0.025\n",
      "episode: 955, score: 0, epsilon: 0.022\n",
      "episode: 960, score: 0, epsilon: 0.020\n",
      "episode: 965, score: 0, epsilon: 0.020\n",
      "episode: 970, score: 0, epsilon: 0.020\n",
      "video: 975, score: 0 (best 700.0 at episode 25)\n",
      "episode: 975, score: 0, epsilon: 0.020\n",
      "episode: 980, score: 0, epsilon: 0.020\n",
      "episode: 985, score: 0, epsilon: 0.020\n",
      "episode: 990, score: 0, epsilon: 0.020\n",
      "episode: 995, score: 0, epsilon: 0.020\n",
      "Matching network values\n",
      "video: 1000, score: 250 (best 700.0 at episode 25)\n",
      "episode: 1000, score: 250, epsilon: 0.020\n",
      "marking, episode: 1000, score: 250.0, mean_score: 48.00, std_score: 105.81\n",
      "episode: 1005, score: 0, epsilon: 0.020\n",
      "episode: 1010, score: 0, epsilon: 0.020\n",
      "episode: 1015, score: 0, epsilon: 0.020\n",
      "episode: 1020, score: 0, epsilon: 0.020\n",
      "video: 1025, score: 0 (best 700.0 at episode 25)\n",
      "episode: 1025, score: 0, epsilon: 0.020\n",
      "episode: 1030, score: 250, epsilon: 0.020\n",
      "episode: 1035, score: 0, epsilon: 0.020\n",
      "episode: 1040, score: 250, epsilon: 0.020\n",
      "episode: 1045, score: 0, epsilon: 0.020\n",
      "video: 1050, score: 0 (best 700.0 at episode 25)\n",
      "episode: 1050, score: 0, epsilon: 0.020\n",
      "episode: 1055, score: 0, epsilon: 0.020\n",
      "episode: 1060, score: 0, epsilon: 0.020\n",
      "episode: 1065, score: 0, epsilon: 0.020\n",
      "episode: 1070, score: 0, epsilon: 0.020\n",
      "video: 1075, score: 0 (best 700.0 at episode 25)\n",
      "episode: 1075, score: 0, epsilon: 0.020\n",
      "episode: 1080, score: 0, epsilon: 0.020\n",
      "episode: 1085, score: 0, epsilon: 0.020\n",
      "episode: 1090, score: 0, epsilon: 0.020\n",
      "episode: 1095, score: 0, epsilon: 0.020\n",
      "Matching network values\n",
      "video: 1100, score: 0 (best 700.0 at episode 25)\n",
      "episode: 1100, score: 0, epsilon: 0.020\n",
      "marking, episode: 1100, score: 0.0, mean_score: 23.00, std_score: 69.07\n",
      "episode: 1105, score: 0, epsilon: 0.020\n",
      "episode: 1110, score: 100, epsilon: 0.020\n",
      "episode: 1115, score: 0, epsilon: 0.020\n",
      "episode: 1120, score: 0, epsilon: 0.020\n",
      "video: 1125, score: 0 (best 700.0 at episode 25)\n",
      "episode: 1125, score: 0, epsilon: 0.020\n",
      "episode: 1130, score: 0, epsilon: 0.020\n",
      "episode: 1135, score: 100, epsilon: 0.020\n",
      "episode: 1140, score: 0, epsilon: 0.020\n",
      "episode: 1145, score: 0, epsilon: 0.020\n",
      "video: 1150, score: 0 (best 700.0 at episode 25)\n",
      "episode: 1150, score: 0, epsilon: 0.020\n",
      "episode: 1155, score: 0, epsilon: 0.020\n",
      "episode: 1160, score: 0, epsilon: 0.020\n",
      "episode: 1165, score: 0, epsilon: 0.020\n",
      "episode: 1170, score: 0, epsilon: 0.020\n",
      "video: 1175, score: 0 (best 700.0 at episode 25)\n",
      "episode: 1175, score: 0, epsilon: 0.020\n",
      "episode: 1180, score: 0, epsilon: 0.020\n",
      "episode: 1185, score: 0, epsilon: 0.020\n",
      "episode: 1190, score: 0, epsilon: 0.020\n",
      "episode: 1195, score: 250, epsilon: 0.020\n",
      "Matching network values\n",
      "video: 1200, score: 250 (best 700.0 at episode 25)\n",
      "episode: 1200, score: 250, epsilon: 0.020\n",
      "marking, episode: 1200, score: 250.0, mean_score: 17.00, std_score: 53.95\n",
      "episode: 1205, score: 0, epsilon: 0.020\n",
      "episode: 1210, score: 100, epsilon: 0.020\n",
      "episode: 1215, score: 0, epsilon: 0.020\n",
      "episode: 1220, score: 0, epsilon: 0.020\n",
      "video: 1225, score: 0 (best 700.0 at episode 25)\n",
      "episode: 1225, score: 0, epsilon: 0.020\n",
      "episode: 1230, score: 0, epsilon: 0.020\n",
      "episode: 1235, score: 0, epsilon: 0.020\n",
      "episode: 1240, score: 0, epsilon: 0.020\n",
      "episode: 1245, score: 0, epsilon: 0.020\n",
      "video: 1250, score: 0 (best 700.0 at episode 25)\n",
      "episode: 1250, score: 0, epsilon: 0.020\n",
      "episode: 1255, score: 0, epsilon: 0.020\n",
      "episode: 1260, score: 0, epsilon: 0.020\n",
      "episode: 1265, score: 0, epsilon: 0.020\n",
      "episode: 1270, score: 100, epsilon: 0.020\n",
      "video: 1275, score: 0 (best 700.0 at episode 25)\n",
      "episode: 1275, score: 0, epsilon: 0.020\n",
      "episode: 1280, score: 100, epsilon: 0.020\n",
      "episode: 1285, score: 0, epsilon: 0.020\n",
      "episode: 1290, score: 200, epsilon: 0.020\n",
      "episode: 1295, score: 0, epsilon: 0.020\n",
      "Matching network values\n",
      "video: 1300, score: 0 (best 700.0 at episode 25)\n",
      "episode: 1300, score: 0, epsilon: 0.020\n",
      "marking, episode: 1300, score: 0.0, mean_score: 11.00, std_score: 43.92\n",
      "episode: 1305, score: 250, epsilon: 0.020\n",
      "episode: 1310, score: 0, epsilon: 0.020\n",
      "episode: 1315, score: 0, epsilon: 0.020\n",
      "episode: 1320, score: 0, epsilon: 0.020\n",
      "video: 1325, score: 0 (best 700.0 at episode 25)\n",
      "episode: 1325, score: 0, epsilon: 0.020\n",
      "episode: 1330, score: 0, epsilon: 0.020\n",
      "episode: 1335, score: 0, epsilon: 0.020\n",
      "episode: 1340, score: 0, epsilon: 0.020\n",
      "episode: 1345, score: 0, epsilon: 0.020\n",
      "video: 1350, score: 250 (best 700.0 at episode 25)\n",
      "episode: 1350, score: 250, epsilon: 0.020\n",
      "episode: 1355, score: 0, epsilon: 0.020\n",
      "episode: 1360, score: 0, epsilon: 0.020\n",
      "episode: 1365, score: 0, epsilon: 0.020\n",
      "episode: 1370, score: 0, epsilon: 0.020\n",
      "video: 1375, score: 0 (best 700.0 at episode 25)\n",
      "episode: 1375, score: 0, epsilon: 0.020\n",
      "episode: 1380, score: 0, epsilon: 0.020\n",
      "episode: 1385, score: 450, epsilon: 0.020\n",
      "episode: 1390, score: 0, epsilon: 0.020\n",
      "episode: 1395, score: 0, epsilon: 0.020\n",
      "Matching network values\n",
      "video: 1400, score: 0 (best 700.0 at episode 25)\n",
      "episode: 1400, score: 0, epsilon: 0.020\n",
      "marking, episode: 1400, score: 0.0, mean_score: 37.00, std_score: 113.05\n",
      "episode: 1405, score: 0, epsilon: 0.020\n",
      "episode: 1410, score: 100, epsilon: 0.020\n",
      "episode: 1415, score: 0, epsilon: 0.020\n",
      "episode: 1420, score: 0, epsilon: 0.020\n",
      "video: 1425, score: 0 (best 700.0 at episode 25)\n",
      "episode: 1425, score: 0, epsilon: 0.020\n",
      "episode: 1430, score: 0, epsilon: 0.020\n",
      "episode: 1435, score: 0, epsilon: 0.020\n",
      "episode: 1440, score: 0, epsilon: 0.020\n",
      "episode: 1445, score: 0, epsilon: 0.020\n",
      "video: 1450, score: 100 (best 700.0 at episode 25)\n",
      "episode: 1450, score: 100, epsilon: 0.020\n",
      "episode: 1455, score: 0, epsilon: 0.020\n",
      "episode: 1460, score: 0, epsilon: 0.020\n",
      "episode: 1465, score: 0, epsilon: 0.020\n",
      "episode: 1470, score: 0, epsilon: 0.020\n",
      "video: 1475, score: 0 (best 700.0 at episode 25)\n",
      "episode: 1475, score: 0, epsilon: 0.020\n",
      "episode: 1480, score: 100, epsilon: 0.020\n",
      "episode: 1485, score: 250, epsilon: 0.020\n",
      "episode: 1490, score: 0, epsilon: 0.020\n",
      "episode: 1495, score: 0, epsilon: 0.020\n",
      "Matching network values\n",
      "video: 1500, score: 0 (best 700.0 at episode 25)\n",
      "episode: 1500, score: 0, epsilon: 0.020\n",
      "marking, episode: 1500, score: 0.0, mean_score: 17.50, std_score: 57.61\n",
      "episode: 1505, score: 0, epsilon: 0.020\n",
      "episode: 1510, score: 0, epsilon: 0.020\n",
      "episode: 1515, score: 0, epsilon: 0.020\n",
      "episode: 1520, score: 0, epsilon: 0.020\n",
      "video: 1525, score: 100 (best 700.0 at episode 25)\n",
      "episode: 1525, score: 100, epsilon: 0.020\n",
      "episode: 1530, score: 0, epsilon: 0.020\n",
      "episode: 1535, score: 0, epsilon: 0.020\n",
      "episode: 1540, score: 0, epsilon: 0.020\n",
      "episode: 1545, score: 0, epsilon: 0.020\n",
      "video: 1550, score: 0 (best 700.0 at episode 25)\n",
      "episode: 1550, score: 0, epsilon: 0.020\n",
      "episode: 1555, score: 0, epsilon: 0.020\n",
      "episode: 1560, score: 0, epsilon: 0.020\n",
      "episode: 1565, score: 0, epsilon: 0.020\n",
      "episode: 1570, score: 0, epsilon: 0.020\n",
      "video: 1575, score: 0 (best 700.0 at episode 25)\n",
      "episode: 1575, score: 0, epsilon: 0.020\n",
      "episode: 1580, score: 0, epsilon: 0.020\n",
      "episode: 1585, score: 100, epsilon: 0.020\n",
      "episode: 1590, score: 0, epsilon: 0.020\n",
      "episode: 1595, score: 0, epsilon: 0.020\n",
      "Matching network values\n",
      "video: 1600, score: 0 (best 700.0 at episode 25)\n",
      "episode: 1600, score: 0, epsilon: 0.020\n",
      "marking, episode: 1600, score: 0.0, mean_score: 34.00, std_score: 102.20\n",
      "episode: 1605, score: 0, epsilon: 0.020\n",
      "episode: 1610, score: 500, epsilon: 0.020\n",
      "episode: 1615, score: 0, epsilon: 0.020\n",
      "episode: 1620, score: 250, epsilon: 0.020\n",
      "video: 1625, score: 0 (best 700.0 at episode 25)\n",
      "episode: 1625, score: 0, epsilon: 0.020\n",
      "episode: 1630, score: 0, epsilon: 0.020\n",
      "episode: 1635, score: 0, epsilon: 0.020\n",
      "episode: 1640, score: 0, epsilon: 0.020\n",
      "episode: 1645, score: 0, epsilon: 0.020\n",
      "video: 1650, score: 0 (best 700.0 at episode 25)\n",
      "episode: 1650, score: 0, epsilon: 0.020\n",
      "episode: 1655, score: 100, epsilon: 0.020\n",
      "episode: 1660, score: 0, epsilon: 0.020\n",
      "episode: 1665, score: 0, epsilon: 0.020\n",
      "episode: 1670, score: 0, epsilon: 0.020\n",
      "video: 1675, score: 0 (best 700.0 at episode 25)\n",
      "episode: 1675, score: 0, epsilon: 0.020\n",
      "episode: 1680, score: 0, epsilon: 0.020\n",
      "episode: 1685, score: 0, epsilon: 0.020\n",
      "episode: 1690, score: 0, epsilon: 0.020\n",
      "episode: 1695, score: 0, epsilon: 0.020\n",
      "Matching network values\n",
      "video: 1700, score: 0 (best 700.0 at episode 25)\n",
      "episode: 1700, score: 0, epsilon: 0.020\n",
      "marking, episode: 1700, score: 0.0, mean_score: 27.50, std_score: 102.80\n",
      "episode: 1705, score: 500, epsilon: 0.020\n",
      "episode: 1710, score: 0, epsilon: 0.020\n",
      "episode: 1715, score: 0, epsilon: 0.020\n",
      "episode: 1720, score: 0, epsilon: 0.020\n",
      "video: 1725, score: 0 (best 700.0 at episode 25)\n",
      "episode: 1725, score: 0, epsilon: 0.020\n",
      "episode: 1730, score: 0, epsilon: 0.020\n",
      "episode: 1735, score: 0, epsilon: 0.020\n",
      "episode: 1740, score: 0, epsilon: 0.020\n",
      "episode: 1745, score: 0, epsilon: 0.020\n",
      "video: 1750, score: 0 (best 700.0 at episode 25)\n",
      "episode: 1750, score: 0, epsilon: 0.020\n",
      "episode: 1755, score: 0, epsilon: 0.020\n",
      "episode: 1760, score: 500, epsilon: 0.020\n",
      "episode: 1765, score: 0, epsilon: 0.020\n",
      "episode: 1770, score: 0, epsilon: 0.020\n",
      "video: 1775, score: 0 (best 700.0 at episode 25)\n",
      "episode: 1775, score: 0, epsilon: 0.020\n",
      "episode: 1780, score: 0, epsilon: 0.020\n",
      "episode: 1785, score: 0, epsilon: 0.020\n",
      "episode: 1790, score: 500, epsilon: 0.020\n",
      "episode: 1795, score: 0, epsilon: 0.020\n",
      "Matching network values\n",
      "video: 1800, score: 0 (best 700.0 at episode 25)\n",
      "episode: 1800, score: 0, epsilon: 0.020\n",
      "marking, episode: 1800, score: 0.0, mean_score: 45.00, std_score: 120.10\n",
      "episode: 1805, score: 0, epsilon: 0.020\n",
      "episode: 1810, score: 0, epsilon: 0.020\n",
      "episode: 1815, score: 0, epsilon: 0.020\n",
      "episode: 1820, score: 500, epsilon: 0.020\n",
      "video: 1825, score: 0 (best 700.0 at episode 25)\n",
      "episode: 1825, score: 0, epsilon: 0.020\n",
      "episode: 1830, score: 0, epsilon: 0.020\n",
      "episode: 1835, score: 0, epsilon: 0.020\n",
      "episode: 1840, score: 0, epsilon: 0.020\n",
      "episode: 1845, score: 0, epsilon: 0.020\n",
      "video: 1850, score: 0 (best 700.0 at episode 25)\n",
      "episode: 1850, score: 0, epsilon: 0.020\n",
      "episode: 1855, score: 0, epsilon: 0.020\n",
      "episode: 1860, score: 0, epsilon: 0.020\n",
      "episode: 1865, score: 0, epsilon: 0.020\n",
      "episode: 1870, score: 0, epsilon: 0.020\n",
      "video: 1875, score: 0 (best 700.0 at episode 25)\n",
      "episode: 1875, score: 0, epsilon: 0.020\n",
      "episode: 1880, score: 250, epsilon: 0.020\n",
      "episode: 1885, score: 0, epsilon: 0.020\n",
      "episode: 1890, score: 0, epsilon: 0.020\n",
      "episode: 1895, score: 0, epsilon: 0.020\n",
      "Matching network values\n",
      "video: 1900, score: 0 (best 700.0 at episode 25)\n",
      "episode: 1900, score: 0, epsilon: 0.020\n",
      "marking, episode: 1900, score: 0.0, mean_score: 30.00, std_score: 119.58\n",
      "episode: 1905, score: 0, epsilon: 0.020\n",
      "episode: 1910, score: 0, epsilon: 0.020\n",
      "episode: 1915, score: 0, epsilon: 0.020\n",
      "episode: 1920, score: 0, epsilon: 0.020\n",
      "video: 1925, score: 0 (best 700.0 at episode 25)\n",
      "episode: 1925, score: 0, epsilon: 0.020\n",
      "episode: 1930, score: 0, epsilon: 0.020\n",
      "episode: 1935, score: 0, epsilon: 0.020\n",
      "episode: 1940, score: 0, epsilon: 0.020\n",
      "episode: 1945, score: 0, epsilon: 0.020\n",
      "video: 1950, score: 0 (best 700.0 at episode 25)\n",
      "episode: 1950, score: 0, epsilon: 0.020\n",
      "episode: 1955, score: 0, epsilon: 0.020\n",
      "episode: 1960, score: 0, epsilon: 0.020\n",
      "episode: 1965, score: 0, epsilon: 0.020\n",
      "episode: 1970, score: 0, epsilon: 0.020\n",
      "video: 1975, score: 250 (best 700.0 at episode 25)\n",
      "episode: 1975, score: 250, epsilon: 0.020\n",
      "episode: 1980, score: 0, epsilon: 0.020\n",
      "episode: 1985, score: 250, epsilon: 0.020\n",
      "episode: 1990, score: 0, epsilon: 0.020\n",
      "episode: 1995, score: 0, epsilon: 0.020\n",
      "Matching network values\n",
      "video: 2000, score: 0 (best 700.0 at episode 25)\n",
      "episode: 2000, score: 0, epsilon: 0.020\n",
      "marking, episode: 2000, score: 0.0, mean_score: 29.50, std_score: 78.45\n",
      "episode: 2005, score: 0, epsilon: 0.020\n",
      "episode: 2010, score: 0, epsilon: 0.020\n",
      "episode: 2015, score: 0, epsilon: 0.020\n",
      "episode: 2020, score: 0, epsilon: 0.020\n",
      "video: 2025, score: 0 (best 700.0 at episode 25)\n",
      "episode: 2025, score: 0, epsilon: 0.020\n",
      "episode: 2030, score: 0, epsilon: 0.020\n",
      "episode: 2035, score: 350, epsilon: 0.020\n",
      "episode: 2040, score: 0, epsilon: 0.020\n",
      "episode: 2045, score: 0, epsilon: 0.020\n",
      "video: 2050, score: 0 (best 700.0 at episode 25)\n",
      "episode: 2050, score: 0, epsilon: 0.020\n",
      "episode: 2055, score: 250, epsilon: 0.020\n",
      "episode: 2060, score: 0, epsilon: 0.020\n",
      "episode: 2065, score: 250, epsilon: 0.020\n",
      "episode: 2070, score: 0, epsilon: 0.020\n",
      "video: 2075, score: 0 (best 700.0 at episode 25)\n",
      "episode: 2075, score: 0, epsilon: 0.020\n",
      "episode: 2080, score: 0, epsilon: 0.020\n",
      "episode: 2085, score: 0, epsilon: 0.020\n",
      "episode: 2090, score: 0, epsilon: 0.020\n",
      "episode: 2095, score: 0, epsilon: 0.020\n",
      "Matching network values\n",
      "video: 2100, score: 0 (best 700.0 at episode 25)\n",
      "episode: 2100, score: 0, epsilon: 0.020\n",
      "marking, episode: 2100, score: 0.0, mean_score: 41.00, std_score: 100.34\n",
      "episode: 2105, score: 0, epsilon: 0.020\n",
      "episode: 2110, score: 100, epsilon: 0.020\n",
      "episode: 2115, score: 250, epsilon: 0.020\n",
      "episode: 2120, score: 0, epsilon: 0.020\n",
      "video: 2125, score: 0 (best 700.0 at episode 25)\n",
      "episode: 2125, score: 0, epsilon: 0.020\n",
      "episode: 2130, score: 0, epsilon: 0.020\n",
      "episode: 2135, score: 0, epsilon: 0.020\n",
      "episode: 2140, score: 0, epsilon: 0.020\n",
      "episode: 2145, score: 0, epsilon: 0.020\n",
      "video: 2150, score: 0 (best 700.0 at episode 25)\n",
      "episode: 2150, score: 0, epsilon: 0.020\n",
      "episode: 2155, score: 0, epsilon: 0.020\n",
      "episode: 2160, score: 0, epsilon: 0.020\n",
      "episode: 2165, score: 0, epsilon: 0.020\n",
      "episode: 2170, score: 0, epsilon: 0.020\n",
      "video: 2175, score: 0 (best 700.0 at episode 25)\n",
      "episode: 2175, score: 0, epsilon: 0.020\n",
      "episode: 2180, score: 250, epsilon: 0.020\n",
      "episode: 2185, score: 0, epsilon: 0.020\n",
      "episode: 2190, score: 0, epsilon: 0.020\n",
      "episode: 2195, score: 250, epsilon: 0.020\n",
      "Matching network values\n",
      "video: 2200, score: 0 (best 700.0 at episode 25)\n",
      "episode: 2200, score: 0, epsilon: 0.020\n",
      "marking, episode: 2200, score: 0.0, mean_score: 34.00, std_score: 77.42\n",
      "episode: 2205, score: 0, epsilon: 0.020\n",
      "episode: 2210, score: 0, epsilon: 0.020\n",
      "episode: 2215, score: 0, epsilon: 0.020\n",
      "episode: 2220, score: 0, epsilon: 0.020\n",
      "video: 2225, score: 0 (best 700.0 at episode 25)\n",
      "episode: 2225, score: 0, epsilon: 0.020\n",
      "episode: 2230, score: 0, epsilon: 0.020\n",
      "episode: 2235, score: 0, epsilon: 0.020\n",
      "episode: 2240, score: 0, epsilon: 0.020\n",
      "episode: 2245, score: 0, epsilon: 0.020\n",
      "video: 2250, score: 0 (best 700.0 at episode 25)\n",
      "episode: 2250, score: 0, epsilon: 0.020\n",
      "episode: 2255, score: 0, epsilon: 0.020\n",
      "episode: 2260, score: 0, epsilon: 0.020\n",
      "episode: 2265, score: 0, epsilon: 0.020\n",
      "episode: 2270, score: 0, epsilon: 0.020\n",
      "video: 2275, score: 350 (best 700.0 at episode 25)\n",
      "episode: 2275, score: 350, epsilon: 0.020\n",
      "episode: 2280, score: 350, epsilon: 0.020\n",
      "episode: 2285, score: 0, epsilon: 0.020\n",
      "episode: 2290, score: 0, epsilon: 0.020\n",
      "episode: 2295, score: 0, epsilon: 0.020\n",
      "Matching network values\n",
      "video: 2300, score: 0 (best 700.0 at episode 25)\n",
      "episode: 2300, score: 0, epsilon: 0.020\n",
      "marking, episode: 2300, score: 0.0, mean_score: 61.00, std_score: 131.45\n",
      "episode: 2305, score: 0, epsilon: 0.020\n",
      "episode: 2310, score: 0, epsilon: 0.020\n",
      "episode: 2315, score: 0, epsilon: 0.020\n",
      "episode: 2320, score: 0, epsilon: 0.020\n",
      "video: 2325, score: 0 (best 700.0 at episode 25)\n",
      "episode: 2325, score: 0, epsilon: 0.020\n",
      "episode: 2330, score: 450, epsilon: 0.020\n",
      "episode: 2335, score: 0, epsilon: 0.020\n",
      "episode: 2340, score: 0, epsilon: 0.020\n",
      "episode: 2345, score: 0, epsilon: 0.020\n",
      "video: 2350, score: 0 (best 700.0 at episode 25)\n",
      "episode: 2350, score: 0, epsilon: 0.020\n",
      "episode: 2355, score: 0, epsilon: 0.020\n",
      "episode: 2360, score: 500, epsilon: 0.020\n",
      "episode: 2365, score: 0, epsilon: 0.020\n",
      "episode: 2370, score: 0, epsilon: 0.020\n",
      "video: 2375, score: 0 (best 700.0 at episode 25)\n",
      "episode: 2375, score: 0, epsilon: 0.020\n",
      "episode: 2380, score: 0, epsilon: 0.020\n",
      "episode: 2385, score: 250, epsilon: 0.020\n",
      "episode: 2390, score: 0, epsilon: 0.020\n",
      "episode: 2395, score: 100, epsilon: 0.020\n",
      "Matching network values\n",
      "video: 2400, score: 0 (best 700.0 at episode 25)\n",
      "episode: 2400, score: 0, epsilon: 0.020\n",
      "marking, episode: 2400, score: 0.0, mean_score: 41.00, std_score: 111.22\n",
      "episode: 2405, score: 0, epsilon: 0.020\n",
      "episode: 2410, score: 0, epsilon: 0.020\n",
      "episode: 2415, score: 250, epsilon: 0.020\n",
      "episode: 2420, score: 0, epsilon: 0.020\n",
      "video: 2425, score: 0 (best 700.0 at episode 25)\n",
      "episode: 2425, score: 0, epsilon: 0.020\n",
      "episode: 2430, score: 0, epsilon: 0.020\n",
      "episode: 2435, score: 0, epsilon: 0.020\n",
      "episode: 2440, score: 100, epsilon: 0.020\n",
      "episode: 2445, score: 0, epsilon: 0.020\n",
      "video: 2450, score: 0 (best 700.0 at episode 25)\n",
      "episode: 2450, score: 0, epsilon: 0.020\n",
      "episode: 2455, score: 0, epsilon: 0.020\n",
      "episode: 2460, score: 0, epsilon: 0.020\n",
      "episode: 2465, score: 0, epsilon: 0.020\n",
      "episode: 2470, score: 0, epsilon: 0.020\n",
      "video: 2475, score: 0 (best 700.0 at episode 25)\n",
      "episode: 2475, score: 0, epsilon: 0.020\n",
      "episode: 2480, score: 0, epsilon: 0.020\n",
      "episode: 2485, score: 350, epsilon: 0.020\n",
      "episode: 2490, score: 0, epsilon: 0.020\n",
      "episode: 2495, score: 0, epsilon: 0.020\n",
      "Matching network values\n",
      "video: 2500, score: 0 (best 700.0 at episode 25)\n",
      "episode: 2500, score: 0, epsilon: 0.020\n",
      "marking, episode: 2500, score: 0.0, mean_score: 43.50, std_score: 96.09\n",
      "episode: 2505, score: 0, epsilon: 0.020\n",
      "episode: 2510, score: 0, epsilon: 0.020\n",
      "episode: 2515, score: 0, epsilon: 0.020\n",
      "episode: 2520, score: 0, epsilon: 0.020\n",
      "video: 2525, score: 0 (best 700.0 at episode 25)\n",
      "episode: 2525, score: 0, epsilon: 0.020\n",
      "episode: 2530, score: 0, epsilon: 0.020\n",
      "episode: 2535, score: 0, epsilon: 0.020\n",
      "episode: 2540, score: 0, epsilon: 0.020\n",
      "episode: 2545, score: 0, epsilon: 0.020\n",
      "video: 2550, score: 0 (best 700.0 at episode 25)\n",
      "episode: 2550, score: 0, epsilon: 0.020\n",
      "episode: 2555, score: 0, epsilon: 0.020\n",
      "episode: 2560, score: 0, epsilon: 0.020\n",
      "episode: 2565, score: 250, epsilon: 0.020\n",
      "episode: 2570, score: 0, epsilon: 0.020\n",
      "video: 2575, score: 0 (best 700.0 at episode 25)\n",
      "episode: 2575, score: 0, epsilon: 0.020\n",
      "episode: 2580, score: 0, epsilon: 0.020\n",
      "episode: 2585, score: 250, epsilon: 0.020\n",
      "episode: 2590, score: 250, epsilon: 0.020\n",
      "episode: 2595, score: 250, epsilon: 0.020\n",
      "Matching network values\n",
      "video: 2600, score: 0 (best 700.0 at episode 25)\n",
      "episode: 2600, score: 0, epsilon: 0.020\n",
      "marking, episode: 2600, score: 0.0, mean_score: 49.50, std_score: 98.11\n",
      "episode: 2605, score: 0, epsilon: 0.020\n",
      "episode: 2610, score: 250, epsilon: 0.020\n",
      "episode: 2615, score: 250, epsilon: 0.020\n",
      "episode: 2620, score: 100, epsilon: 0.020\n",
      "video: 2625, score: 0 (best 700.0 at episode 25)\n",
      "episode: 2625, score: 0, epsilon: 0.020\n",
      "episode: 2630, score: 0, epsilon: 0.020\n",
      "episode: 2635, score: 0, epsilon: 0.020\n",
      "episode: 2640, score: 0, epsilon: 0.020\n",
      "episode: 2645, score: 0, epsilon: 0.020\n",
      "video: 2650, score: 0 (best 700.0 at episode 25)\n",
      "episode: 2650, score: 0, epsilon: 0.020\n",
      "episode: 2655, score: 0, epsilon: 0.020\n",
      "episode: 2660, score: 0, epsilon: 0.020\n",
      "episode: 2665, score: 0, epsilon: 0.020\n",
      "episode: 2670, score: 0, epsilon: 0.020\n",
      "video: 2675, score: 0 (best 700.0 at episode 25)\n",
      "episode: 2675, score: 0, epsilon: 0.020\n",
      "episode: 2680, score: 0, epsilon: 0.020\n",
      "episode: 2685, score: 0, epsilon: 0.020\n",
      "episode: 2690, score: 0, epsilon: 0.020\n",
      "episode: 2695, score: 0, epsilon: 0.020\n",
      "Matching network values\n",
      "video: 2700, score: 0 (best 700.0 at episode 25)\n",
      "episode: 2700, score: 0, epsilon: 0.020\n",
      "marking, episode: 2700, score: 0.0, mean_score: 51.00, std_score: 102.22\n",
      "episode: 2705, score: 0, epsilon: 0.020\n",
      "episode: 2710, score: 0, epsilon: 0.020\n",
      "episode: 2715, score: 0, epsilon: 0.020\n",
      "episode: 2720, score: 0, epsilon: 0.020\n",
      "video: 2725, score: 0 (best 700.0 at episode 25)\n",
      "episode: 2725, score: 0, epsilon: 0.020\n",
      "episode: 2730, score: 0, epsilon: 0.020\n",
      "episode: 2735, score: 100, epsilon: 0.020\n",
      "episode: 2740, score: 0, epsilon: 0.020\n",
      "episode: 2745, score: 0, epsilon: 0.020\n",
      "video: 2750, score: 0 (best 700.0 at episode 25)\n",
      "episode: 2750, score: 0, epsilon: 0.020\n",
      "episode: 2755, score: 0, epsilon: 0.020\n",
      "episode: 2760, score: 250, epsilon: 0.020\n",
      "episode: 2765, score: 0, epsilon: 0.020\n",
      "episode: 2770, score: 0, epsilon: 0.020\n",
      "video: 2775, score: 0 (best 700.0 at episode 25)\n",
      "episode: 2775, score: 0, epsilon: 0.020\n",
      "episode: 2780, score: 0, epsilon: 0.020\n",
      "episode: 2785, score: 0, epsilon: 0.020\n",
      "episode: 2790, score: 0, epsilon: 0.020\n",
      "episode: 2795, score: 0, epsilon: 0.020\n",
      "Matching network values\n",
      "video: 2800, score: 100 (best 700.0 at episode 25)\n",
      "episode: 2800, score: 100, epsilon: 0.020\n",
      "marking, episode: 2800, score: 100.0, mean_score: 62.00, std_score: 133.63\n",
      "episode: 2805, score: 0, epsilon: 0.020\n",
      "episode: 2810, score: 100, epsilon: 0.020\n",
      "episode: 2815, score: 0, epsilon: 0.020\n",
      "episode: 2820, score: 100, epsilon: 0.020\n",
      "video: 2825, score: 0 (best 700.0 at episode 25)\n",
      "episode: 2825, score: 0, epsilon: 0.020\n",
      "episode: 2830, score: 0, epsilon: 0.020\n",
      "episode: 2835, score: 0, epsilon: 0.020\n",
      "episode: 2840, score: 0, epsilon: 0.020\n",
      "episode: 2845, score: 0, epsilon: 0.020\n",
      "video: 2850, score: 0 (best 700.0 at episode 25)\n",
      "episode: 2850, score: 0, epsilon: 0.020\n",
      "episode: 2855, score: 0, epsilon: 0.020\n",
      "episode: 2860, score: 250, epsilon: 0.020\n",
      "episode: 2865, score: 0, epsilon: 0.020\n",
      "episode: 2870, score: 0, epsilon: 0.020\n",
      "video: 2875, score: 0 (best 700.0 at episode 25)\n",
      "episode: 2875, score: 0, epsilon: 0.020\n",
      "episode: 2880, score: 0, epsilon: 0.020\n",
      "episode: 2885, score: 0, epsilon: 0.020\n",
      "episode: 2890, score: 0, epsilon: 0.020\n",
      "episode: 2895, score: 250, epsilon: 0.020\n",
      "Matching network values\n",
      "video: 2900, score: 0 (best 700.0 at episode 25)\n",
      "episode: 2900, score: 0, epsilon: 0.020\n",
      "marking, episode: 2900, score: 0.0, mean_score: 45.00, std_score: 109.43\n",
      "episode: 2905, score: 100, epsilon: 0.020\n",
      "episode: 2910, score: 0, epsilon: 0.020\n",
      "episode: 2915, score: 0, epsilon: 0.020\n",
      "episode: 2920, score: 0, epsilon: 0.020\n",
      "video: 2925, score: 250 (best 700.0 at episode 25)\n",
      "episode: 2925, score: 250, epsilon: 0.020\n",
      "episode: 2930, score: 0, epsilon: 0.020\n",
      "episode: 2935, score: 0, epsilon: 0.020\n",
      "episode: 2940, score: 0, epsilon: 0.020\n",
      "episode: 2945, score: 0, epsilon: 0.020\n",
      "video: 2950, score: 500 (best 700.0 at episode 25)\n",
      "episode: 2950, score: 500, epsilon: 0.020\n",
      "episode: 2955, score: 500, epsilon: 0.020\n",
      "episode: 2960, score: 0, epsilon: 0.020\n",
      "episode: 2965, score: 250, epsilon: 0.020\n",
      "episode: 2970, score: 350, epsilon: 0.020\n",
      "video: 2975, score: 250 (best 700.0 at episode 25)\n",
      "episode: 2975, score: 250, epsilon: 0.020\n",
      "episode: 2980, score: 250, epsilon: 0.020\n",
      "episode: 2985, score: 0, epsilon: 0.020\n",
      "episode: 2990, score: 350, epsilon: 0.020\n",
      "episode: 2995, score: 0, epsilon: 0.020\n",
      "Matching network values\n",
      "video: 3000, score: 100 (best 700.0 at episode 25)\n",
      "episode: 3000, score: 100, epsilon: 0.020\n",
      "marking, episode: 3000, score: 100.0, mean_score: 88.50, std_score: 149.31\n",
      "episode: 3005, score: 250, epsilon: 0.020\n",
      "episode: 3010, score: 0, epsilon: 0.020\n",
      "episode: 3015, score: 500, epsilon: 0.020\n",
      "episode: 3020, score: 250, epsilon: 0.020\n",
      "video: 3025, score: 250 (best 700.0 at episode 25)\n",
      "episode: 3025, score: 250, epsilon: 0.020\n",
      "episode: 3030, score: 0, epsilon: 0.020\n",
      "episode: 3035, score: 0, epsilon: 0.020\n",
      "episode: 3040, score: 0, epsilon: 0.020\n",
      "episode: 3045, score: 500, epsilon: 0.020\n",
      "video: 3050, score: 250 (best 700.0 at episode 25)\n",
      "episode: 3050, score: 250, epsilon: 0.020\n",
      "episode: 3055, score: 100, epsilon: 0.020\n",
      "episode: 3060, score: 0, epsilon: 0.020\n",
      "episode: 3065, score: 0, epsilon: 0.020\n",
      "episode: 3070, score: 0, epsilon: 0.020\n",
      "video: 3075, score: 100 (best 700.0 at episode 25)\n",
      "episode: 3075, score: 100, epsilon: 0.020\n",
      "episode: 3080, score: 0, epsilon: 0.020\n",
      "episode: 3085, score: 250, epsilon: 0.020\n",
      "episode: 3090, score: 0, epsilon: 0.020\n",
      "episode: 3095, score: 0, epsilon: 0.020\n",
      "Matching network values\n",
      "video: 3100, score: 0 (best 700.0 at episode 25)\n",
      "episode: 3100, score: 0, epsilon: 0.020\n",
      "marking, episode: 3100, score: 0.0, mean_score: 102.00, std_score: 150.98\n",
      "episode: 3105, score: 0, epsilon: 0.020\n",
      "episode: 3110, score: 250, epsilon: 0.020\n",
      "episode: 3115, score: 0, epsilon: 0.020\n",
      "episode: 3120, score: 0, epsilon: 0.020\n",
      "video: 3125, score: 250 (best 700.0 at episode 25)\n",
      "episode: 3125, score: 250, epsilon: 0.020\n",
      "episode: 3130, score: 0, epsilon: 0.020\n",
      "episode: 3135, score: 250, epsilon: 0.020\n",
      "episode: 3140, score: 0, epsilon: 0.020\n",
      "episode: 3145, score: 100, epsilon: 0.020\n",
      "video: 3150, score: 0 (best 700.0 at episode 25)\n",
      "episode: 3150, score: 0, epsilon: 0.020\n",
      "episode: 3155, score: 0, epsilon: 0.020\n",
      "episode: 3160, score: 100, epsilon: 0.020\n",
      "episode: 3165, score: 250, epsilon: 0.020\n",
      "episode: 3170, score: 0, epsilon: 0.020\n",
      "video: 3175, score: 0 (best 700.0 at episode 25)\n",
      "episode: 3175, score: 0, epsilon: 0.020\n",
      "episode: 3180, score: 250, epsilon: 0.020\n",
      "episode: 3185, score: 0, epsilon: 0.020\n",
      "episode: 3190, score: 250, epsilon: 0.020\n",
      "episode: 3195, score: 0, epsilon: 0.020\n",
      "Matching network values\n",
      "video: 3200, score: 250 (best 700.0 at episode 25)\n",
      "episode: 3200, score: 250, epsilon: 0.020\n",
      "marking, episode: 3200, score: 250.0, mean_score: 86.50, std_score: 159.82\n",
      "episode: 3205, score: 0, epsilon: 0.020\n",
      "episode: 3210, score: 0, epsilon: 0.020\n",
      "episode: 3215, score: 0, epsilon: 0.020\n",
      "episode: 3220, score: 750, epsilon: 0.020\n",
      "video: 3225, score: 0 (best 700.0 at episode 25)\n",
      "episode: 3225, score: 0, epsilon: 0.020\n",
      "episode: 3230, score: 250, epsilon: 0.020\n",
      "episode: 3235, score: 0, epsilon: 0.020\n",
      "episode: 3240, score: 0, epsilon: 0.020\n",
      "episode: 3245, score: 0, epsilon: 0.020\n",
      "video: 3250, score: 0 (best 700.0 at episode 25)\n",
      "episode: 3250, score: 0, epsilon: 0.020\n",
      "episode: 3255, score: 0, epsilon: 0.020\n",
      "episode: 3260, score: 0, epsilon: 0.020\n",
      "episode: 3265, score: 0, epsilon: 0.020\n",
      "episode: 3270, score: 0, epsilon: 0.020\n",
      "video: 3275, score: 0 (best 700.0 at episode 25)\n",
      "episode: 3275, score: 0, epsilon: 0.020\n",
      "episode: 3280, score: 0, epsilon: 0.020\n",
      "episode: 3285, score: 0, epsilon: 0.020\n",
      "episode: 3290, score: 250, epsilon: 0.020\n",
      "episode: 3295, score: 250, epsilon: 0.020\n",
      "Matching network values\n",
      "video: 3300, score: 0 (best 700.0 at episode 25)\n",
      "episode: 3300, score: 0, epsilon: 0.020\n",
      "marking, episode: 3300, score: 0.0, mean_score: 79.00, std_score: 158.62\n",
      "episode: 3305, score: 0, epsilon: 0.020\n",
      "episode: 3310, score: 0, epsilon: 0.020\n",
      "episode: 3315, score: 0, epsilon: 0.020\n",
      "episode: 3320, score: 0, epsilon: 0.020\n",
      "video: 3325, score: 0 (best 700.0 at episode 25)\n",
      "episode: 3325, score: 0, epsilon: 0.020\n",
      "episode: 3330, score: 0, epsilon: 0.020\n",
      "episode: 3335, score: 0, epsilon: 0.020\n",
      "episode: 3340, score: 100, epsilon: 0.020\n",
      "episode: 3345, score: 100, epsilon: 0.020\n",
      "video: 3350, score: 0 (best 700.0 at episode 25)\n",
      "episode: 3350, score: 0, epsilon: 0.020\n",
      "episode: 3355, score: 0, epsilon: 0.020\n",
      "episode: 3360, score: 0, epsilon: 0.020\n",
      "episode: 3365, score: 0, epsilon: 0.020\n",
      "episode: 3370, score: 0, epsilon: 0.020\n",
      "video: 3375, score: 100 (best 700.0 at episode 25)\n",
      "episode: 3375, score: 100, epsilon: 0.020\n",
      "episode: 3380, score: 0, epsilon: 0.020\n",
      "episode: 3385, score: 250, epsilon: 0.020\n",
      "episode: 3390, score: 0, epsilon: 0.020\n",
      "episode: 3395, score: 0, epsilon: 0.020\n",
      "Matching network values\n",
      "video: 3400, score: 0 (best 700.0 at episode 25)\n",
      "episode: 3400, score: 0, epsilon: 0.020\n",
      "marking, episode: 3400, score: 0.0, mean_score: 50.50, std_score: 123.79\n",
      "episode: 3405, score: 200, epsilon: 0.020\n",
      "episode: 3410, score: 100, epsilon: 0.020\n",
      "episode: 3415, score: 250, epsilon: 0.020\n",
      "episode: 3420, score: 0, epsilon: 0.020\n",
      "video: 3425, score: 0 (best 700.0 at episode 25)\n",
      "episode: 3425, score: 0, epsilon: 0.020\n",
      "episode: 3430, score: 0, epsilon: 0.020\n",
      "episode: 3435, score: 0, epsilon: 0.020\n",
      "episode: 3440, score: 0, epsilon: 0.020\n",
      "episode: 3445, score: 500, epsilon: 0.020\n",
      "video: 3450, score: 0 (best 700.0 at episode 25)\n",
      "episode: 3450, score: 0, epsilon: 0.020\n",
      "episode: 3455, score: 0, epsilon: 0.020\n",
      "episode: 3460, score: 0, epsilon: 0.020\n",
      "episode: 3465, score: 0, epsilon: 0.020\n",
      "episode: 3470, score: 0, epsilon: 0.020\n",
      "video: 3475, score: 100 (best 700.0 at episode 25)\n",
      "episode: 3475, score: 100, epsilon: 0.020\n",
      "episode: 3480, score: 0, epsilon: 0.020\n",
      "episode: 3485, score: 0, epsilon: 0.020\n",
      "episode: 3490, score: 100, epsilon: 0.020\n",
      "episode: 3495, score: 0, epsilon: 0.020\n",
      "Matching network values\n",
      "video: 3500, score: 0 (best 700.0 at episode 25)\n",
      "episode: 3500, score: 0, epsilon: 0.020\n",
      "marking, episode: 3500, score: 0.0, mean_score: 54.00, std_score: 119.10\n",
      "episode: 3505, score: 100, epsilon: 0.020\n",
      "episode: 3510, score: 0, epsilon: 0.020\n",
      "episode: 3515, score: 0, epsilon: 0.020\n",
      "episode: 3520, score: 100, epsilon: 0.020\n",
      "video: 3525, score: 0 (best 700.0 at episode 25)\n",
      "episode: 3525, score: 0, epsilon: 0.020\n",
      "episode: 3530, score: 450, epsilon: 0.020\n",
      "episode: 3535, score: 0, epsilon: 0.020\n",
      "episode: 3540, score: 0, epsilon: 0.020\n",
      "episode: 3545, score: 0, epsilon: 0.020\n",
      "video: 3550, score: 350 (best 700.0 at episode 25)\n",
      "episode: 3550, score: 350, epsilon: 0.020\n",
      "episode: 3555, score: 0, epsilon: 0.020\n",
      "episode: 3560, score: 0, epsilon: 0.020\n",
      "episode: 3565, score: 100, epsilon: 0.020\n",
      "episode: 3570, score: 0, epsilon: 0.020\n",
      "video: 3575, score: 0 (best 700.0 at episode 25)\n",
      "episode: 3575, score: 0, epsilon: 0.020\n",
      "episode: 3580, score: 0, epsilon: 0.020\n",
      "episode: 3585, score: 500, epsilon: 0.020\n",
      "episode: 3590, score: 0, epsilon: 0.020\n",
      "episode: 3595, score: 0, epsilon: 0.020\n",
      "Matching network values\n",
      "video: 3600, score: 0 (best 700.0 at episode 25)\n",
      "episode: 3600, score: 0, epsilon: 0.020\n",
      "marking, episode: 3600, score: 0.0, mean_score: 71.00, std_score: 127.31\n",
      "episode: 3605, score: 0, epsilon: 0.020\n",
      "episode: 3610, score: 0, epsilon: 0.020\n",
      "episode: 3615, score: 0, epsilon: 0.020\n",
      "episode: 3620, score: 0, epsilon: 0.020\n",
      "video: 3625, score: 0 (best 700.0 at episode 25)\n",
      "episode: 3625, score: 0, epsilon: 0.020\n",
      "episode: 3630, score: 0, epsilon: 0.020\n",
      "episode: 3635, score: 0, epsilon: 0.020\n",
      "episode: 3640, score: 0, epsilon: 0.020\n",
      "episode: 3645, score: 0, epsilon: 0.020\n",
      "video: 3650, score: 0 (best 700.0 at episode 25)\n",
      "episode: 3650, score: 0, epsilon: 0.020\n",
      "episode: 3655, score: 100, epsilon: 0.020\n",
      "episode: 3660, score: 0, epsilon: 0.020\n",
      "episode: 3665, score: 250, epsilon: 0.020\n",
      "episode: 3670, score: 350, epsilon: 0.020\n",
      "video: 3675, score: 100 (best 700.0 at episode 25)\n",
      "episode: 3675, score: 100, epsilon: 0.020\n",
      "episode: 3680, score: 250, epsilon: 0.020\n",
      "episode: 3685, score: 100, epsilon: 0.020\n",
      "episode: 3690, score: 0, epsilon: 0.020\n",
      "episode: 3695, score: 100, epsilon: 0.020\n",
      "Matching network values\n",
      "video: 3700, score: 0 (best 700.0 at episode 25)\n",
      "episode: 3700, score: 0, epsilon: 0.020\n",
      "marking, episode: 3700, score: 0.0, mean_score: 59.00, std_score: 115.41\n",
      "episode: 3705, score: 0, epsilon: 0.020\n",
      "episode: 3710, score: 0, epsilon: 0.020\n",
      "episode: 3715, score: 0, epsilon: 0.020\n",
      "episode: 3720, score: 0, epsilon: 0.020\n",
      "video: 3725, score: 600 (best 700.0 at episode 25)\n",
      "episode: 3725, score: 600, epsilon: 0.020\n",
      "episode: 3730, score: 350, epsilon: 0.020\n",
      "episode: 3735, score: 0, epsilon: 0.020\n",
      "episode: 3740, score: 0, epsilon: 0.020\n",
      "episode: 3745, score: 0, epsilon: 0.020\n",
      "video: 3750, score: 250 (best 700.0 at episode 25)\n",
      "episode: 3750, score: 250, epsilon: 0.020\n",
      "episode: 3755, score: 0, epsilon: 0.020\n",
      "episode: 3760, score: 100, epsilon: 0.020\n",
      "episode: 3765, score: 250, epsilon: 0.020\n",
      "episode: 3770, score: 0, epsilon: 0.020\n",
      "video: 3775, score: 0 (best 700.0 at episode 25)\n",
      "episode: 3775, score: 0, epsilon: 0.020\n",
      "episode: 3780, score: 350, epsilon: 0.020\n",
      "episode: 3785, score: 0, epsilon: 0.020\n",
      "episode: 3790, score: 0, epsilon: 0.020\n",
      "episode: 3795, score: 0, epsilon: 0.020\n",
      "Matching network values\n",
      "video: 3800, score: 0 (best 700.0 at episode 25)\n",
      "episode: 3800, score: 0, epsilon: 0.020\n",
      "marking, episode: 3800, score: 0.0, mean_score: 75.50, std_score: 130.67\n",
      "episode: 3805, score: 0, epsilon: 0.020\n",
      "episode: 3810, score: 0, epsilon: 0.020\n",
      "episode: 3815, score: 100, epsilon: 0.020\n",
      "episode: 3820, score: 0, epsilon: 0.020\n",
      "video: 3825, score: 0 (best 700.0 at episode 25)\n",
      "episode: 3825, score: 0, epsilon: 0.020\n",
      "episode: 3830, score: 100, epsilon: 0.020\n",
      "episode: 3835, score: 0, epsilon: 0.020\n",
      "episode: 3840, score: 250, epsilon: 0.020\n",
      "episode: 3845, score: 0, epsilon: 0.020\n",
      "video: 3850, score: 0 (best 700.0 at episode 25)\n",
      "episode: 3850, score: 0, epsilon: 0.020\n",
      "episode: 3855, score: 0, epsilon: 0.020\n",
      "episode: 3860, score: 0, epsilon: 0.020\n",
      "episode: 3865, score: 0, epsilon: 0.020\n",
      "episode: 3870, score: 100, epsilon: 0.020\n",
      "video: 3875, score: 0 (best 700.0 at episode 25)\n",
      "episode: 3875, score: 0, epsilon: 0.020\n",
      "episode: 3880, score: 0, epsilon: 0.020\n",
      "episode: 3885, score: 0, epsilon: 0.020\n",
      "episode: 3890, score: 0, epsilon: 0.020\n",
      "episode: 3895, score: 0, epsilon: 0.020\n",
      "Matching network values\n",
      "video: 3900, score: 0 (best 700.0 at episode 25)\n",
      "episode: 3900, score: 0, epsilon: 0.020\n",
      "marking, episode: 3900, score: 0.0, mean_score: 70.00, std_score: 142.48\n",
      "episode: 3905, score: 0, epsilon: 0.020\n",
      "episode: 3910, score: 0, epsilon: 0.020\n",
      "episode: 3915, score: 0, epsilon: 0.020\n",
      "episode: 3920, score: 0, epsilon: 0.020\n",
      "video: 3925, score: 0 (best 700.0 at episode 25)\n",
      "episode: 3925, score: 0, epsilon: 0.020\n",
      "episode: 3930, score: 0, epsilon: 0.020\n",
      "episode: 3935, score: 0, epsilon: 0.020\n",
      "episode: 3940, score: 0, epsilon: 0.020\n",
      "episode: 3945, score: 0, epsilon: 0.020\n",
      "video: 3950, score: 0 (best 700.0 at episode 25)\n",
      "episode: 3950, score: 0, epsilon: 0.020\n",
      "episode: 3955, score: 0, epsilon: 0.020\n",
      "episode: 3960, score: 0, epsilon: 0.020\n",
      "episode: 3965, score: 0, epsilon: 0.020\n",
      "episode: 3970, score: 0, epsilon: 0.020\n",
      "video: 3975, score: 0 (best 700.0 at episode 25)\n",
      "episode: 3975, score: 0, epsilon: 0.020\n",
      "episode: 3980, score: 0, epsilon: 0.020\n",
      "episode: 3985, score: 0, epsilon: 0.020\n",
      "episode: 3990, score: 0, epsilon: 0.020\n",
      "episode: 3995, score: 0, epsilon: 0.020\n",
      "Matching network values\n",
      "video: 4000, score: 0 (best 700.0 at episode 25)\n",
      "episode: 4000, score: 0, epsilon: 0.020\n",
      "marking, episode: 4000, score: 0.0, mean_score: 36.50, std_score: 85.40\n",
      "episode: 4005, score: 0, epsilon: 0.020\n",
      "episode: 4010, score: 0, epsilon: 0.020\n",
      "episode: 4015, score: 500, epsilon: 0.020\n",
      "episode: 4020, score: 0, epsilon: 0.020\n",
      "video: 4025, score: 100 (best 700.0 at episode 25)\n",
      "episode: 4025, score: 100, epsilon: 0.020\n",
      "episode: 4030, score: 250, epsilon: 0.020\n",
      "episode: 4035, score: 350, epsilon: 0.020\n",
      "episode: 4040, score: 100, epsilon: 0.020\n",
      "episode: 4045, score: 0, epsilon: 0.020\n",
      "video: 4050, score: 0 (best 700.0 at episode 25)\n",
      "episode: 4050, score: 0, epsilon: 0.020\n",
      "episode: 4055, score: 0, epsilon: 0.020\n",
      "episode: 4060, score: 0, epsilon: 0.020\n",
      "episode: 4065, score: 250, epsilon: 0.020\n",
      "episode: 4070, score: 0, epsilon: 0.020\n",
      "video: 4075, score: 0 (best 700.0 at episode 25)\n",
      "episode: 4075, score: 0, epsilon: 0.020\n",
      "episode: 4080, score: 500, epsilon: 0.020\n",
      "episode: 4085, score: 0, epsilon: 0.020\n",
      "episode: 4090, score: 0, epsilon: 0.020\n",
      "episode: 4095, score: 0, epsilon: 0.020\n",
      "Matching network values\n",
      "video: 4100, score: 100 (best 700.0 at episode 25)\n",
      "episode: 4100, score: 100, epsilon: 0.020\n",
      "marking, episode: 4100, score: 100.0, mean_score: 102.50, std_score: 141.31\n",
      "episode: 4105, score: 0, epsilon: 0.020\n",
      "episode: 4110, score: 100, epsilon: 0.020\n",
      "episode: 4115, score: 100, epsilon: 0.020\n",
      "episode: 4120, score: 0, epsilon: 0.020\n",
      "video: 4125, score: 0 (best 700.0 at episode 25)\n",
      "episode: 4125, score: 0, epsilon: 0.020\n",
      "episode: 4130, score: 0, epsilon: 0.020\n",
      "episode: 4135, score: 100, epsilon: 0.020\n",
      "episode: 4140, score: 0, epsilon: 0.020\n",
      "episode: 4145, score: 100, epsilon: 0.020\n",
      "video: 4150, score: 0 (best 700.0 at episode 25)\n",
      "episode: 4150, score: 0, epsilon: 0.020\n",
      "episode: 4155, score: 750, epsilon: 0.020\n",
      "episode: 4160, score: 0, epsilon: 0.020\n",
      "episode: 4165, score: 0, epsilon: 0.020\n",
      "episode: 4170, score: 100, epsilon: 0.020\n",
      "video: 4175, score: 0 (best 700.0 at episode 25)\n",
      "episode: 4175, score: 0, epsilon: 0.020\n",
      "episode: 4180, score: 100, epsilon: 0.020\n",
      "episode: 4185, score: 250, epsilon: 0.020\n",
      "episode: 4190, score: 0, epsilon: 0.020\n",
      "episode: 4195, score: 200, epsilon: 0.020\n",
      "Matching network values\n",
      "video: 4200, score: 0 (best 700.0 at episode 25)\n",
      "episode: 4200, score: 0, epsilon: 0.020\n",
      "marking, episode: 4200, score: 0.0, mean_score: 61.50, std_score: 111.10\n",
      "episode: 4205, score: 250, epsilon: 0.020\n",
      "episode: 4210, score: 0, epsilon: 0.020\n",
      "episode: 4215, score: 100, epsilon: 0.020\n",
      "episode: 4220, score: 0, epsilon: 0.020\n",
      "video: 4225, score: 0 (best 700.0 at episode 25)\n",
      "episode: 4225, score: 0, epsilon: 0.020\n",
      "episode: 4230, score: 250, epsilon: 0.020\n",
      "episode: 4235, score: 100, epsilon: 0.020\n",
      "episode: 4240, score: 0, epsilon: 0.020\n",
      "episode: 4245, score: 0, epsilon: 0.020\n",
      "video: 4250, score: 0 (best 700.0 at episode 25)\n",
      "episode: 4250, score: 0, epsilon: 0.020\n",
      "episode: 4255, score: 100, epsilon: 0.020\n",
      "episode: 4260, score: 0, epsilon: 0.020\n",
      "episode: 4265, score: 0, epsilon: 0.020\n",
      "episode: 4270, score: 0, epsilon: 0.020\n",
      "video: 4275, score: 200 (best 700.0 at episode 25)\n",
      "episode: 4275, score: 200, epsilon: 0.020\n",
      "episode: 4280, score: 0, epsilon: 0.020\n",
      "episode: 4285, score: 250, epsilon: 0.020\n",
      "episode: 4290, score: 0, epsilon: 0.020\n",
      "episode: 4295, score: 0, epsilon: 0.020\n",
      "Matching network values\n",
      "video: 4300, score: 0 (best 700.0 at episode 25)\n",
      "episode: 4300, score: 0, epsilon: 0.020\n",
      "marking, episode: 4300, score: 0.0, mean_score: 98.00, std_score: 162.93\n",
      "episode: 4305, score: 100, epsilon: 0.020\n",
      "episode: 4310, score: 0, epsilon: 0.020\n",
      "episode: 4315, score: 0, epsilon: 0.020\n",
      "episode: 4320, score: 0, epsilon: 0.020\n",
      "video: 4325, score: 250 (best 700.0 at episode 25)\n",
      "episode: 4325, score: 250, epsilon: 0.020\n",
      "episode: 4330, score: 0, epsilon: 0.020\n",
      "episode: 4335, score: 500, epsilon: 0.020\n",
      "episode: 4340, score: 600, epsilon: 0.020\n",
      "episode: 4345, score: 250, epsilon: 0.020\n",
      "video: 4350, score: 0 (best 700.0 at episode 25)\n",
      "episode: 4350, score: 0, epsilon: 0.020\n",
      "episode: 4355, score: 0, epsilon: 0.020\n",
      "episode: 4360, score: 100, epsilon: 0.020\n",
      "episode: 4365, score: 250, epsilon: 0.020\n",
      "episode: 4370, score: 0, epsilon: 0.020\n",
      "video: 4375, score: 0 (best 700.0 at episode 25)\n",
      "episode: 4375, score: 0, epsilon: 0.020\n",
      "episode: 4380, score: 350, epsilon: 0.020\n",
      "episode: 4385, score: 100, epsilon: 0.020\n",
      "episode: 4390, score: 0, epsilon: 0.020\n",
      "episode: 4395, score: 250, epsilon: 0.020\n",
      "Matching network values\n",
      "video: 4400, score: 0 (best 700.0 at episode 25)\n",
      "episode: 4400, score: 0, epsilon: 0.020\n",
      "marking, episode: 4400, score: 0.0, mean_score: 88.50, std_score: 149.47\n",
      "episode: 4405, score: 0, epsilon: 0.020\n",
      "episode: 4410, score: 0, epsilon: 0.020\n",
      "episode: 4415, score: 0, epsilon: 0.020\n",
      "episode: 4420, score: 0, epsilon: 0.020\n",
      "video: 4425, score: 100 (best 700.0 at episode 25)\n",
      "episode: 4425, score: 100, epsilon: 0.020\n",
      "episode: 4430, score: 0, epsilon: 0.020\n",
      "episode: 4435, score: 100, epsilon: 0.020\n",
      "episode: 4440, score: 100, epsilon: 0.020\n",
      "episode: 4445, score: 0, epsilon: 0.020\n",
      "video: 4450, score: 0 (best 700.0 at episode 25)\n",
      "episode: 4450, score: 0, epsilon: 0.020\n",
      "episode: 4455, score: 0, epsilon: 0.020\n",
      "episode: 4460, score: 0, epsilon: 0.020\n",
      "episode: 4465, score: 250, epsilon: 0.020\n",
      "episode: 4470, score: 250, epsilon: 0.020\n",
      "video: 4475, score: 250 (best 700.0 at episode 25)\n",
      "episode: 4475, score: 250, epsilon: 0.020\n",
      "episode: 4480, score: 250, epsilon: 0.020\n",
      "episode: 4485, score: 100, epsilon: 0.020\n",
      "episode: 4490, score: 0, epsilon: 0.020\n",
      "episode: 4495, score: 0, epsilon: 0.020\n",
      "Matching network values\n",
      "video: 4500, score: 0 (best 700.0 at episode 25)\n",
      "episode: 4500, score: 0, epsilon: 0.020\n",
      "marking, episode: 4500, score: 0.0, mean_score: 60.00, std_score: 124.50\n",
      "episode: 4505, score: 0, epsilon: 0.020\n",
      "episode: 4510, score: 0, epsilon: 0.020\n",
      "episode: 4515, score: 0, epsilon: 0.020\n",
      "episode: 4520, score: 250, epsilon: 0.020\n",
      "video: 4525, score: 350 (best 700.0 at episode 25)\n",
      "episode: 4525, score: 350, epsilon: 0.020\n",
      "episode: 4530, score: 0, epsilon: 0.020\n",
      "episode: 4535, score: 500, epsilon: 0.020\n",
      "episode: 4540, score: 0, epsilon: 0.020\n",
      "episode: 4545, score: 0, epsilon: 0.020\n",
      "video: 4550, score: 100 (best 700.0 at episode 25)\n",
      "episode: 4550, score: 100, epsilon: 0.020\n",
      "episode: 4555, score: 0, epsilon: 0.020\n",
      "episode: 4560, score: 250, epsilon: 0.020\n",
      "episode: 4565, score: 250, epsilon: 0.020\n",
      "episode: 4570, score: 0, epsilon: 0.020\n",
      "video: 4575, score: 0 (best 700.0 at episode 25)\n",
      "episode: 4575, score: 0, epsilon: 0.020\n",
      "episode: 4580, score: 0, epsilon: 0.020\n",
      "episode: 4585, score: 0, epsilon: 0.020\n",
      "episode: 4590, score: 0, epsilon: 0.020\n",
      "episode: 4595, score: 0, epsilon: 0.020\n",
      "Matching network values\n",
      "video: 4600, score: 0 (best 700.0 at episode 25)\n",
      "episode: 4600, score: 0, epsilon: 0.020\n",
      "marking, episode: 4600, score: 0.0, mean_score: 75.50, std_score: 139.19\n",
      "episode: 4605, score: 0, epsilon: 0.020\n",
      "episode: 4610, score: 500, epsilon: 0.020\n",
      "episode: 4615, score: 0, epsilon: 0.020\n",
      "episode: 4620, score: 0, epsilon: 0.020\n",
      "video: 4625, score: 100 (best 700.0 at episode 25)\n",
      "episode: 4625, score: 100, epsilon: 0.020\n",
      "episode: 4630, score: 0, epsilon: 0.020\n",
      "episode: 4635, score: 0, epsilon: 0.020\n",
      "episode: 4640, score: 0, epsilon: 0.020\n",
      "episode: 4645, score: 0, epsilon: 0.020\n",
      "video: 4650, score: 0 (best 700.0 at episode 25)\n",
      "episode: 4650, score: 0, epsilon: 0.020\n",
      "episode: 4655, score: 0, epsilon: 0.020\n",
      "episode: 4660, score: 100, epsilon: 0.020\n",
      "episode: 4665, score: 100, epsilon: 0.020\n",
      "episode: 4670, score: 0, epsilon: 0.020\n",
      "video: 4675, score: 0 (best 700.0 at episode 25)\n",
      "episode: 4675, score: 0, epsilon: 0.020\n",
      "episode: 4680, score: 0, epsilon: 0.020\n",
      "episode: 4685, score: 0, epsilon: 0.020\n",
      "episode: 4690, score: 0, epsilon: 0.020\n",
      "episode: 4695, score: 0, epsilon: 0.020\n",
      "Matching network values\n",
      "video: 4700, score: 250 (best 700.0 at episode 25)\n",
      "episode: 4700, score: 250, epsilon: 0.020\n",
      "marking, episode: 4700, score: 250.0, mean_score: 93.50, std_score: 170.68\n",
      "episode: 4705, score: 0, epsilon: 0.020\n",
      "episode: 4710, score: 0, epsilon: 0.020\n",
      "episode: 4715, score: 0, epsilon: 0.020\n",
      "episode: 4720, score: 0, epsilon: 0.020\n",
      "video: 4725, score: 250 (best 700.0 at episode 25)\n",
      "episode: 4725, score: 250, epsilon: 0.020\n",
      "episode: 4730, score: 0, epsilon: 0.020\n",
      "episode: 4735, score: 0, epsilon: 0.020\n",
      "episode: 4740, score: 0, epsilon: 0.020\n",
      "episode: 4745, score: 0, epsilon: 0.020\n",
      "video: 4750, score: 0 (best 700.0 at episode 25)\n",
      "episode: 4750, score: 0, epsilon: 0.020\n",
      "episode: 4755, score: 0, epsilon: 0.020\n",
      "episode: 4760, score: 0, epsilon: 0.020\n",
      "episode: 4765, score: 0, epsilon: 0.020\n",
      "episode: 4770, score: 0, epsilon: 0.020\n",
      "video: 4775, score: 250 (best 700.0 at episode 25)\n",
      "episode: 4775, score: 250, epsilon: 0.020\n",
      "episode: 4780, score: 0, epsilon: 0.020\n",
      "episode: 4785, score: 0, epsilon: 0.020\n",
      "episode: 4790, score: 0, epsilon: 0.020\n",
      "episode: 4795, score: 250, epsilon: 0.020\n",
      "Matching network values\n",
      "video: 4800, score: 0 (best 700.0 at episode 25)\n",
      "episode: 4800, score: 0, epsilon: 0.020\n",
      "marking, episode: 4800, score: 0.0, mean_score: 97.50, std_score: 156.90\n",
      "episode: 4805, score: 0, epsilon: 0.020\n",
      "episode: 4810, score: 0, epsilon: 0.020\n",
      "episode: 4815, score: 350, epsilon: 0.020\n",
      "episode: 4820, score: 0, epsilon: 0.020\n",
      "video: 4825, score: 250 (best 700.0 at episode 25)\n",
      "episode: 4825, score: 250, epsilon: 0.020\n",
      "episode: 4830, score: 0, epsilon: 0.020\n",
      "episode: 4835, score: 0, epsilon: 0.020\n",
      "episode: 4840, score: 100, epsilon: 0.020\n",
      "episode: 4845, score: 0, epsilon: 0.020\n",
      "video: 4850, score: 0 (best 700.0 at episode 25)\n",
      "episode: 4850, score: 0, epsilon: 0.020\n",
      "episode: 4855, score: 600, epsilon: 0.020\n",
      "episode: 4860, score: 0, epsilon: 0.020\n",
      "episode: 4865, score: 0, epsilon: 0.020\n",
      "episode: 4870, score: 250, epsilon: 0.020\n",
      "video: 4875, score: 0 (best 700.0 at episode 25)\n",
      "episode: 4875, score: 0, epsilon: 0.020\n",
      "episode: 4880, score: 350, epsilon: 0.020\n",
      "episode: 4885, score: 0, epsilon: 0.020\n",
      "episode: 4890, score: 0, epsilon: 0.020\n",
      "episode: 4895, score: 0, epsilon: 0.020\n",
      "Matching network values\n",
      "video: 4900, score: 100 (best 700.0 at episode 25)\n",
      "episode: 4900, score: 100, epsilon: 0.020\n",
      "marking, episode: 4900, score: 100.0, mean_score: 88.50, std_score: 153.44\n",
      "episode: 4905, score: 0, epsilon: 0.020\n",
      "episode: 4910, score: 0, epsilon: 0.020\n",
      "episode: 4915, score: 0, epsilon: 0.020\n",
      "episode: 4920, score: 0, epsilon: 0.020\n",
      "video: 4925, score: 0 (best 700.0 at episode 25)\n",
      "episode: 4925, score: 0, epsilon: 0.020\n",
      "episode: 4930, score: 0, epsilon: 0.020\n",
      "episode: 4935, score: 0, epsilon: 0.020\n",
      "episode: 4940, score: 250, epsilon: 0.020\n",
      "episode: 4945, score: 0, epsilon: 0.020\n",
      "video: 4950, score: 0 (best 700.0 at episode 25)\n",
      "episode: 4950, score: 0, epsilon: 0.020\n",
      "episode: 4955, score: 0, epsilon: 0.020\n",
      "episode: 4960, score: 250, epsilon: 0.020\n",
      "episode: 4965, score: 350, epsilon: 0.020\n",
      "episode: 4970, score: 250, epsilon: 0.020\n",
      "video: 4975, score: 350 (best 700.0 at episode 25)\n",
      "episode: 4975, score: 350, epsilon: 0.020\n",
      "episode: 4980, score: 0, epsilon: 0.020\n",
      "episode: 4985, score: 0, epsilon: 0.020\n",
      "episode: 4990, score: 350, epsilon: 0.020\n",
      "episode: 4995, score: 100, epsilon: 0.020\n",
      "Matching network values\n",
      "video: 5000, score: 250 (best 700.0 at episode 25)\n",
      "episode: 5000, score: 250, epsilon: 0.020\n",
      "marking, episode: 5000, score: 250.0, mean_score: 78.00, std_score: 146.34\n",
      "episode: 5005, score: 0, epsilon: 0.020\n",
      "episode: 5010, score: 0, epsilon: 0.020\n",
      "episode: 5015, score: 0, epsilon: 0.020\n",
      "episode: 5020, score: 0, epsilon: 0.020\n",
      "video: 5025, score: 0 (best 700.0 at episode 25)\n",
      "episode: 5025, score: 0, epsilon: 0.020\n",
      "episode: 5030, score: 0, epsilon: 0.020\n",
      "episode: 5035, score: 0, epsilon: 0.020\n",
      "episode: 5040, score: 0, epsilon: 0.020\n",
      "episode: 5045, score: 0, epsilon: 0.020\n",
      "video: 5050, score: 0 (best 700.0 at episode 25)\n",
      "episode: 5050, score: 0, epsilon: 0.020\n",
      "episode: 5055, score: 0, epsilon: 0.020\n",
      "episode: 5060, score: 0, epsilon: 0.020\n",
      "episode: 5065, score: 350, epsilon: 0.020\n",
      "episode: 5070, score: 0, epsilon: 0.020\n",
      "video: 5075, score: 0 (best 700.0 at episode 25)\n",
      "episode: 5075, score: 0, epsilon: 0.020\n",
      "episode: 5080, score: 100, epsilon: 0.020\n",
      "episode: 5085, score: 500, epsilon: 0.020\n",
      "episode: 5090, score: 0, epsilon: 0.020\n",
      "episode: 5095, score: 0, epsilon: 0.020\n",
      "Matching network values\n",
      "video: 5100, score: 0 (best 700.0 at episode 25)\n",
      "episode: 5100, score: 0, epsilon: 0.020\n",
      "marking, episode: 5100, score: 0.0, mean_score: 81.00, std_score: 135.05\n",
      "episode: 5105, score: 250, epsilon: 0.020\n",
      "episode: 5110, score: 0, epsilon: 0.020\n",
      "episode: 5115, score: 250, epsilon: 0.020\n",
      "episode: 5120, score: 100, epsilon: 0.020\n",
      "video: 5125, score: 250 (best 700.0 at episode 25)\n",
      "episode: 5125, score: 250, epsilon: 0.020\n",
      "episode: 5130, score: 0, epsilon: 0.020\n",
      "episode: 5135, score: 0, epsilon: 0.020\n",
      "episode: 5140, score: 0, epsilon: 0.020\n",
      "episode: 5145, score: 0, epsilon: 0.020\n",
      "video: 5150, score: 0 (best 700.0 at episode 25)\n",
      "episode: 5150, score: 0, epsilon: 0.020\n",
      "episode: 5155, score: 0, epsilon: 0.020\n",
      "episode: 5160, score: 0, epsilon: 0.020\n",
      "episode: 5165, score: 0, epsilon: 0.020\n",
      "episode: 5170, score: 100, epsilon: 0.020\n",
      "video: 5175, score: 0 (best 700.0 at episode 25)\n",
      "episode: 5175, score: 0, epsilon: 0.020\n",
      "episode: 5180, score: 0, epsilon: 0.020\n",
      "episode: 5185, score: 0, epsilon: 0.020\n",
      "episode: 5190, score: 0, epsilon: 0.020\n",
      "episode: 5195, score: 500, epsilon: 0.020\n",
      "Matching network values\n",
      "video: 5200, score: 0 (best 700.0 at episode 25)\n",
      "episode: 5200, score: 0, epsilon: 0.020\n",
      "marking, episode: 5200, score: 0.0, mean_score: 73.50, std_score: 137.92\n",
      "episode: 5205, score: 100, epsilon: 0.020\n",
      "episode: 5210, score: 100, epsilon: 0.020\n",
      "episode: 5215, score: 200, epsilon: 0.020\n",
      "episode: 5220, score: 0, epsilon: 0.020\n",
      "video: 5225, score: 250 (best 700.0 at episode 25)\n",
      "episode: 5225, score: 250, epsilon: 0.020\n",
      "episode: 5230, score: 0, epsilon: 0.020\n",
      "episode: 5235, score: 100, epsilon: 0.020\n",
      "episode: 5240, score: 100, epsilon: 0.020\n",
      "episode: 5245, score: 0, epsilon: 0.020\n",
      "video: 5250, score: 0 (best 700.0 at episode 25)\n",
      "episode: 5250, score: 0, epsilon: 0.020\n",
      "episode: 5255, score: 0, epsilon: 0.020\n",
      "episode: 5260, score: 250, epsilon: 0.020\n",
      "episode: 5265, score: 250, epsilon: 0.020\n",
      "episode: 5270, score: 0, epsilon: 0.020\n",
      "video: 5275, score: 0 (best 700.0 at episode 25)\n",
      "episode: 5275, score: 0, epsilon: 0.020\n",
      "episode: 5280, score: 200, epsilon: 0.020\n",
      "episode: 5285, score: 350, epsilon: 0.020\n",
      "episode: 5290, score: 0, epsilon: 0.020\n",
      "episode: 5295, score: 0, epsilon: 0.020\n",
      "Matching network values\n",
      "video: 5300, score: 0 (best 700.0 at episode 25)\n",
      "episode: 5300, score: 0, epsilon: 0.020\n",
      "marking, episode: 5300, score: 0.0, mean_score: 78.00, std_score: 111.87\n",
      "episode: 5305, score: 0, epsilon: 0.020\n",
      "episode: 5310, score: 100, epsilon: 0.020\n",
      "episode: 5315, score: 100, epsilon: 0.020\n",
      "episode: 5320, score: 0, epsilon: 0.020\n",
      "video: 5325, score: 0 (best 700.0 at episode 25)\n",
      "episode: 5325, score: 0, epsilon: 0.020\n",
      "episode: 5330, score: 0, epsilon: 0.020\n",
      "episode: 5335, score: 250, epsilon: 0.020\n",
      "episode: 5340, score: 0, epsilon: 0.020\n",
      "episode: 5345, score: 250, epsilon: 0.020\n",
      "video: 5350, score: 0 (best 700.0 at episode 25)\n",
      "episode: 5350, score: 0, epsilon: 0.020\n",
      "episode: 5355, score: 0, epsilon: 0.020\n",
      "episode: 5360, score: 0, epsilon: 0.020\n",
      "episode: 5365, score: 0, epsilon: 0.020\n",
      "episode: 5370, score: 0, epsilon: 0.020\n",
      "video: 5375, score: 350 (best 700.0 at episode 25)\n",
      "episode: 5375, score: 350, epsilon: 0.020\n",
      "episode: 5380, score: 0, epsilon: 0.020\n",
      "episode: 5385, score: 250, epsilon: 0.020\n",
      "episode: 5390, score: 250, epsilon: 0.020\n",
      "episode: 5395, score: 0, epsilon: 0.020\n",
      "Matching network values\n",
      "video: 5400, score: 350 (best 700.0 at episode 25)\n",
      "episode: 5400, score: 350, epsilon: 0.020\n",
      "marking, episode: 5400, score: 350.0, mean_score: 72.00, std_score: 133.85\n",
      "episode: 5405, score: 0, epsilon: 0.020\n",
      "episode: 5410, score: 0, epsilon: 0.020\n",
      "episode: 5415, score: 0, epsilon: 0.020\n",
      "episode: 5420, score: 0, epsilon: 0.020\n",
      "video: 5425, score: 250 (best 700.0 at episode 25)\n",
      "episode: 5425, score: 250, epsilon: 0.020\n",
      "episode: 5430, score: 0, epsilon: 0.020\n",
      "episode: 5435, score: 100, epsilon: 0.020\n",
      "episode: 5440, score: 0, epsilon: 0.020\n",
      "episode: 5445, score: 100, epsilon: 0.020\n",
      "video: 5450, score: 250 (best 700.0 at episode 25)\n",
      "episode: 5450, score: 250, epsilon: 0.020\n",
      "episode: 5455, score: 500, epsilon: 0.020\n",
      "episode: 5460, score: 100, epsilon: 0.020\n",
      "episode: 5465, score: 0, epsilon: 0.020\n",
      "episode: 5470, score: 0, epsilon: 0.020\n",
      "video: 5475, score: 0 (best 700.0 at episode 25)\n",
      "episode: 5475, score: 0, epsilon: 0.020\n",
      "episode: 5480, score: 0, epsilon: 0.020\n",
      "episode: 5485, score: 0, epsilon: 0.020\n",
      "episode: 5490, score: 0, epsilon: 0.020\n",
      "episode: 5495, score: 0, epsilon: 0.020\n",
      "Matching network values\n",
      "video: 5500, score: 0 (best 700.0 at episode 25)\n",
      "episode: 5500, score: 0, epsilon: 0.020\n",
      "marking, episode: 5500, score: 0.0, mean_score: 71.50, std_score: 131.58\n",
      "episode: 5505, score: 0, epsilon: 0.020\n",
      "episode: 5510, score: 0, epsilon: 0.020\n",
      "episode: 5515, score: 450, epsilon: 0.020\n",
      "episode: 5520, score: 0, epsilon: 0.020\n",
      "video: 5525, score: 100 (best 700.0 at episode 25)\n",
      "episode: 5525, score: 100, epsilon: 0.020\n",
      "episode: 5530, score: 0, epsilon: 0.020\n",
      "episode: 5535, score: 0, epsilon: 0.020\n",
      "episode: 5540, score: 0, epsilon: 0.020\n",
      "episode: 5545, score: 250, epsilon: 0.020\n",
      "video: 5550, score: 0 (best 700.0 at episode 25)\n",
      "episode: 5550, score: 0, epsilon: 0.020\n",
      "episode: 5555, score: 250, epsilon: 0.020\n",
      "episode: 5560, score: 100, epsilon: 0.020\n",
      "episode: 5565, score: 100, epsilon: 0.020\n",
      "episode: 5570, score: 0, epsilon: 0.020\n",
      "video: 5575, score: 0 (best 700.0 at episode 25)\n",
      "episode: 5575, score: 0, epsilon: 0.020\n",
      "episode: 5580, score: 0, epsilon: 0.020\n",
      "episode: 5585, score: 0, epsilon: 0.020\n",
      "episode: 5590, score: 0, epsilon: 0.020\n",
      "episode: 5595, score: 0, epsilon: 0.020\n",
      "Matching network values\n",
      "video: 5600, score: 0 (best 700.0 at episode 25)\n",
      "episode: 5600, score: 0, epsilon: 0.020\n",
      "marking, episode: 5600, score: 0.0, mean_score: 59.00, std_score: 108.94\n",
      "episode: 5605, score: 250, epsilon: 0.020\n",
      "episode: 5610, score: 0, epsilon: 0.020\n",
      "episode: 5615, score: 0, epsilon: 0.020\n",
      "episode: 5620, score: 0, epsilon: 0.020\n",
      "video: 5625, score: 0 (best 700.0 at episode 25)\n",
      "episode: 5625, score: 0, epsilon: 0.020\n",
      "episode: 5630, score: 250, epsilon: 0.020\n",
      "episode: 5635, score: 0, epsilon: 0.020\n",
      "episode: 5640, score: 100, epsilon: 0.020\n",
      "episode: 5645, score: 0, epsilon: 0.020\n",
      "video: 5650, score: 100 (best 700.0 at episode 25)\n",
      "episode: 5650, score: 100, epsilon: 0.020\n",
      "episode: 5655, score: 0, epsilon: 0.020\n",
      "episode: 5660, score: 0, epsilon: 0.020\n",
      "episode: 5665, score: 0, epsilon: 0.020\n",
      "episode: 5670, score: 500, epsilon: 0.020\n",
      "video: 5675, score: 0 (best 700.0 at episode 25)\n",
      "episode: 5675, score: 0, epsilon: 0.020\n",
      "episode: 5680, score: 0, epsilon: 0.020\n",
      "episode: 5685, score: 250, epsilon: 0.020\n",
      "episode: 5690, score: 0, epsilon: 0.020\n",
      "episode: 5695, score: 250, epsilon: 0.020\n",
      "Matching network values\n",
      "video: 5700, score: 0 (best 700.0 at episode 25)\n",
      "episode: 5700, score: 0, epsilon: 0.020\n",
      "marking, episode: 5700, score: 0.0, mean_score: 85.00, std_score: 131.24\n",
      "episode: 5705, score: 350, epsilon: 0.020\n",
      "episode: 5710, score: 0, epsilon: 0.020\n",
      "episode: 5715, score: 250, epsilon: 0.020\n",
      "episode: 5720, score: 100, epsilon: 0.020\n",
      "video: 5725, score: 0 (best 700.0 at episode 25)\n",
      "episode: 5725, score: 0, epsilon: 0.020\n",
      "episode: 5730, score: 0, epsilon: 0.020\n",
      "episode: 5735, score: 250, epsilon: 0.020\n",
      "episode: 5740, score: 0, epsilon: 0.020\n",
      "episode: 5745, score: 0, epsilon: 0.020\n",
      "video: 5750, score: 250 (best 700.0 at episode 25)\n",
      "episode: 5750, score: 250, epsilon: 0.020\n",
      "episode: 5755, score: 0, epsilon: 0.020\n",
      "episode: 5760, score: 500, epsilon: 0.020\n",
      "episode: 5765, score: 100, epsilon: 0.020\n",
      "episode: 5770, score: 0, epsilon: 0.020\n",
      "video: 5775, score: 350 (best 700.0 at episode 25)\n",
      "episode: 5775, score: 350, epsilon: 0.020\n",
      "episode: 5780, score: 250, epsilon: 0.020\n",
      "episode: 5785, score: 0, epsilon: 0.020\n",
      "episode: 5790, score: 0, epsilon: 0.020\n",
      "episode: 5795, score: 250, epsilon: 0.020\n",
      "Matching network values\n",
      "video: 5800, score: 0 (best 700.0 at episode 25)\n",
      "episode: 5800, score: 0, epsilon: 0.020\n",
      "marking, episode: 5800, score: 0.0, mean_score: 115.00, std_score: 163.02\n",
      "episode: 5805, score: 0, epsilon: 0.020\n",
      "episode: 5810, score: 600, epsilon: 0.020\n",
      "episode: 5815, score: 0, epsilon: 0.020\n",
      "episode: 5820, score: 250, epsilon: 0.020\n",
      "video: 5825, score: 0 (best 700.0 at episode 25)\n",
      "episode: 5825, score: 0, epsilon: 0.020\n",
      "episode: 5830, score: 0, epsilon: 0.020\n",
      "episode: 5835, score: 100, epsilon: 0.020\n",
      "episode: 5840, score: 0, epsilon: 0.020\n",
      "episode: 5845, score: 0, epsilon: 0.020\n",
      "video: 5850, score: 0 (best 700.0 at episode 25)\n",
      "episode: 5850, score: 0, epsilon: 0.020\n",
      "episode: 5855, score: 0, epsilon: 0.020\n",
      "episode: 5860, score: 0, epsilon: 0.020\n",
      "episode: 5865, score: 100, epsilon: 0.020\n",
      "episode: 5870, score: 0, epsilon: 0.020\n",
      "video: 5875, score: 0 (best 700.0 at episode 25)\n",
      "episode: 5875, score: 0, epsilon: 0.020\n",
      "episode: 5880, score: 0, epsilon: 0.020\n",
      "episode: 5885, score: 0, epsilon: 0.020\n",
      "episode: 5890, score: 0, epsilon: 0.020\n",
      "episode: 5895, score: 0, epsilon: 0.020\n",
      "Matching network values\n",
      "video: 5900, score: 100 (best 700.0 at episode 25)\n",
      "episode: 5900, score: 100, epsilon: 0.020\n",
      "marking, episode: 5900, score: 100.0, mean_score: 43.00, std_score: 106.78\n",
      "episode: 5905, score: 0, epsilon: 0.020\n",
      "episode: 5910, score: 100, epsilon: 0.020\n",
      "episode: 5915, score: 0, epsilon: 0.020\n",
      "episode: 5920, score: 0, epsilon: 0.020\n",
      "video: 5925, score: 100 (best 700.0 at episode 25)\n",
      "episode: 5925, score: 100, epsilon: 0.020\n",
      "episode: 5930, score: 500, epsilon: 0.020\n",
      "episode: 5935, score: 0, epsilon: 0.020\n",
      "episode: 5940, score: 100, epsilon: 0.020\n",
      "episode: 5945, score: 0, epsilon: 0.020\n",
      "video: 5950, score: 0 (best 700.0 at episode 25)\n",
      "episode: 5950, score: 0, epsilon: 0.020\n",
      "episode: 5955, score: 0, epsilon: 0.020\n",
      "episode: 5960, score: 0, epsilon: 0.020\n",
      "episode: 5965, score: 0, epsilon: 0.020\n",
      "episode: 5970, score: 0, epsilon: 0.020\n",
      "video: 5975, score: 0 (best 700.0 at episode 25)\n",
      "episode: 5975, score: 0, epsilon: 0.020\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-7-1b2282b12de5>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mmain\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m \u001B[1;31m# %lprun -f ReplayBuffer.put -f ReplayBuffer.sample -f Agent.train_step -f main main()\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-6-7bd536f22131>\u001B[0m in \u001B[0;36mmain\u001B[1;34m()\u001B[0m\n\u001B[0;32m     62\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     63\u001B[0m                 \u001B[1;31m# RND intrinsic reward calculation\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 64\u001B[1;33m                 \u001B[0mintrinsic_reward\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0magent\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mintrinsic_reward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mstate_\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0munsqueeze\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfloat\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     65\u001B[0m                 \u001B[0mintrinsic_rewards\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mintrinsic_reward\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     66\u001B[0m                 \u001B[0mstd\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstd\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mintrinsic_rewards\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-5-a33255adb4bf>\u001B[0m in \u001B[0;36mintrinsic_reward\u001B[1;34m(self, state_)\u001B[0m\n\u001B[0;32m     92\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     93\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mintrinsic_reward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstate_\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 94\u001B[1;33m         \u001B[0mtarget_next_feature\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mpredicted_next_feature\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrnd\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mstate_\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     95\u001B[0m         \u001B[0mintrinsic_reward\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mtarget_next_feature\u001B[0m \u001B[1;33m-\u001B[0m \u001B[0mpredicted_next_feature\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpow\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m2\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msum\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m-\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m*\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mintrinsic_reward_factor\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     96\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mintrinsic_reward\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcpu\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdetach\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mitem\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-5-a33255adb4bf>\u001B[0m in \u001B[0;36mrnd\u001B[1;34m(self, state_)\u001B[0m\n\u001B[0;32m     87\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     88\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mrnd\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstate_\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 89\u001B[1;33m         \u001B[0mtarget_next_feature\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mf_target\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mstate_\u001B[0m \u001B[1;33m*\u001B[0m \u001B[0mSTATE_VALUE_FACTOR\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     90\u001B[0m         \u001B[0mpredicted_next_feature\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mf_predictor\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mstate_\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     91\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mtarget_next_feature\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mpredicted_next_feature\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Users\\Max\\PycharmProjects\\dlrl-coursework\\envs\\dlrl-coursework\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m    725\u001B[0m             \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_slow_forward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    726\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 727\u001B[1;33m             \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mforward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    728\u001B[0m         for hook in itertools.chain(\n\u001B[0;32m    729\u001B[0m                 \u001B[0m_global_forward_hooks\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-4-7c68663fc518>\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m    121\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mATARI_IMAGE_ENV\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    122\u001B[0m             \u001B[0mx\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mconv\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 123\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfc\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mview\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m-\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    124\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Users\\Max\\PycharmProjects\\dlrl-coursework\\envs\\dlrl-coursework\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m    725\u001B[0m             \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_slow_forward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    726\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 727\u001B[1;33m             \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mforward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    728\u001B[0m         for hook in itertools.chain(\n\u001B[0;32m    729\u001B[0m                 \u001B[0m_global_forward_hooks\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Users\\Max\\PycharmProjects\\dlrl-coursework\\envs\\dlrl-coursework\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    115\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minput\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    116\u001B[0m         \u001B[1;32mfor\u001B[0m \u001B[0mmodule\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 117\u001B[1;33m             \u001B[0minput\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmodule\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    118\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0minput\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    119\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Users\\Max\\PycharmProjects\\dlrl-coursework\\envs\\dlrl-coursework\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m    725\u001B[0m             \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_slow_forward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    726\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 727\u001B[1;33m             \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mforward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    728\u001B[0m         for hook in itertools.chain(\n\u001B[0;32m    729\u001B[0m                 \u001B[0m_global_forward_hooks\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Users\\Max\\PycharmProjects\\dlrl-coursework\\envs\\dlrl-coursework\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m     91\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     92\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minput\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mTensor\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m->\u001B[0m \u001B[0mTensor\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 93\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mF\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlinear\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mweight\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbias\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     94\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     95\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mextra_repr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m->\u001B[0m \u001B[0mstr\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Users\\Max\\PycharmProjects\\dlrl-coursework\\envs\\dlrl-coursework\\lib\\site-packages\\torch\\nn\\functional.py\u001B[0m in \u001B[0;36mlinear\u001B[1;34m(input, weight, bias)\u001B[0m\n\u001B[0;32m   1688\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0minput\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdim\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;36m2\u001B[0m \u001B[1;32mand\u001B[0m \u001B[0mbias\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1689\u001B[0m         \u001B[1;31m# fused op is marginally faster\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1690\u001B[1;33m         \u001B[0mret\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0maddmm\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mbias\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mweight\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mt\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1691\u001B[0m     \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1692\u001B[0m         \u001B[0moutput\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0minput\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmatmul\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mweight\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mt\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "main()\n",
    "# %lprun -f ReplayBuffer.put -f ReplayBuffer.sample -f Agent.train_step -f main main()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}